{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e2fb2d-0092-4f93-80fa-7bd9aa67ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib2to3.pytree import WildcardPattern\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "import argparse\n",
    "import bz2\n",
    "import logging\n",
    "import os.path\n",
    "import re  # TODO use regex when it will be standard\n",
    "import sys\n",
    "from io import StringIO\n",
    "from multiprocessing import Queue, get_context, cpu_count\n",
    "from timeit import default_timer\n",
    "import re\n",
    "import pymorphy2\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from ruwordnet import RuWordNet\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from  extractor import Extractor\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81554125-25c3-4800-b936-7210814d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        if i!= \"(\" and i != \")\":\n",
    "            s +=i\n",
    "        elif i  == \")\":\n",
    "            s += \"\"\n",
    "        else:\n",
    "            s += \",\"\n",
    "    return s\n",
    "@lru_cache(maxsize=200000)\n",
    "def get_normal_form(word):\n",
    "    return morph_analizer.parse(word)[0].normal_form\n",
    "acceptedNamespaces = ['w', 'wiktionary', 'wikt']\n",
    "templateNamespace = ''\n",
    "morph_analizer = pymorphy2.MorphAnalyzer()\n",
    "tagRE = re.compile(r'(.*?)<(/?\\w+)[^>]*>(?:([^<]*)(<.*?>)?)?')\n",
    "def collect_pages(text):\n",
    "    \"\"\"\n",
    "    :param text: the text of a wikipedia file dump.\n",
    "    \"\"\"\n",
    "    # we collect individual lines, since str.join() is significantly faster\n",
    "    # than concatenation\n",
    "    page = []\n",
    "    id = ''\n",
    "    revid = ''\n",
    "    last_id = ''\n",
    "    inText = False\n",
    "    redirect = False\n",
    "    redirect_page = ''\n",
    "    for line in text:\n",
    "        if '<' not in line:     # faster than doing re.search()\n",
    "            if inText:\n",
    "                page.append(line)\n",
    "            continue\n",
    "        m = tagRE.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        tag = m.group(2)\n",
    "        if tag == 'page':\n",
    "            page = []\n",
    "            redirect = False\n",
    "        elif tag == 'id' and not id:\n",
    "            id = m.group(3)\n",
    "        elif tag == 'id' and id: # <revision> <id></id> </revision>\n",
    "            revid = m.group(3)\n",
    "        elif tag == 'title':\n",
    "            title = m.group(3)\n",
    "        elif tag == 'redirect':\n",
    "            redirect = True\n",
    "            redirectRE = re.compile(r'title=\\\"(.*?)\\\" />')\n",
    "            redirect_page = re.findall(redirectRE, line)[0]\n",
    "        elif tag == 'text':\n",
    "            inText = True\n",
    "            line = line[m.start(3):m.end(3)]\n",
    "            page.append(line)\n",
    "            if m.lastindex == 4:  # open-close\n",
    "                inText = False\n",
    "        elif tag == '/text':\n",
    "            if m.group(1):\n",
    "                page.append(m.group(1))\n",
    "            inText = False\n",
    "        elif inText:\n",
    "            page.append(line)\n",
    "        elif tag == '/page':\n",
    "            colon = title.find(':')\n",
    "            if (colon < 0 or (title[:colon] in acceptedNamespaces) and id != last_id and\n",
    "                    not redirect and not title.startswith(templateNamespace)):\n",
    "                yield (id, revid, title, page, redirect_page,redirect)\n",
    "                last_id = id\n",
    "            id = ''\n",
    "            revid = ''\n",
    "            page = []\n",
    "            inText = False\n",
    "            redirect = False\n",
    "            redirect_page=''\n",
    "\n",
    "def decode_open(filename, mode='rt', encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Open a file, decode and decompress, depending on extension `gz`, or 'bz2`.\n",
    "    :param filename: the file to open.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(filename)[1]\n",
    "    if ext == '.gz':\n",
    "        import gzip\n",
    "        return gzip.open(filename, mode, encoding=encoding)\n",
    "    elif ext == '.bz2':\n",
    "        return bz2.open(filename, mode=mode, encoding=encoding)\n",
    "    else:\n",
    "        return open(filename, mode, encoding=encoding)\n",
    "def extract_cat(text):\n",
    "    matcher=re.compile(r\"Категория:\\s?([А-Яа-я\\s?]+)\")\n",
    "    return matcher.findall(text)\n",
    "def extract_links(text):\n",
    "    matcher=re.compile(r\"[\\[\\[]([А-Яа-я\\s?]+)[\\|,\\]\\]]\")\n",
    "    return matcher.findall(text)\n",
    "def extract_first_links(text):\n",
    "    matcher=re.compile(r\"[\\[\\[]([А-Яа-я\\s?]+)[\\|,\\]\\]]\")\n",
    "    answer = []\n",
    "    for elem in text.split(\"\\n\"):\n",
    "        item = matcher.findall(elem)\n",
    "        if len(item) > 0:\n",
    "            answer.append(item[0])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dda5bd4-fd2d-4fbb-8387-b0515260a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Page:\n",
    "    id: int\n",
    "    revid: int\n",
    "    title:str\n",
    "    meaningPage: bool\n",
    "    multiPage: bool\n",
    "    categories: list\n",
    "    links: list\n",
    "    redirect:bool\n",
    "    first_sentence:str\n",
    "    def __eq__(self, other):\n",
    "        return (self.id, self.revid, self.title, self.meaningPage,\n",
    "                self.multiPage, self.categories, self.links, \n",
    "                self.redirect) == (other.id, other.revid, \n",
    "                other.title, other.meaningPage,\n",
    "                other.multiPage, other.categories, other.links, \n",
    "                other.redirect)\n",
    "class PageEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Page):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "matcher=re.compile(r\"[A-Яа-яё]+-?[A-Яа-яё]\")\n",
    "def extract_clear_text(text):\n",
    "    \"\"\"извлечение  слов из текста и приведение их к нормальному виду\n",
    "    вовзраащемое значение строка из нормализированных слов(знаки препинания удалены)\"\"\"\n",
    "    return \" \".join([get_normal_form(x) for x in matcher.findall(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af38d96d-e401-4bb1-a6e1-14fa2f86a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4432391it [56:05, 1317.11it/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12184\\466410804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m#     if title == \"Abbath\" or title == \"Аббате\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m#         print(text, file = log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\lbase_data\\\\ctxw.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "input = decode_open(\"D:\\\\asd\\\\ruwiki-20220701-pages-meta-current.xml.bz2\")\n",
    "i = 0\n",
    "dictRedirect = {}\n",
    "pages = []\n",
    "redirectcount = 0\n",
    "dictPageRedirect = {}\n",
    "# log = open(\"log.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for id, revid, title, page, redirect_page, redirect in tqdm(collect_pages(input)):\n",
    "    # print(title, \"aaaa\",''.join(page), file = printt)\n",
    "    i += 1\n",
    "    text = ''.join(page)\n",
    "    text_lower = text.lower()\n",
    "    multiPage = False\n",
    "    if text_lower.find('{{другие значения') != -1:\n",
    "        multiPage = True\n",
    "    elif title.find(\"(\") != -1 and (not \"значения\" in title.lower())and (not \"значение\" in title.lower()):\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{перенаправление\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{другое значение\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{значения\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{redirect-multi\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{redirect-multi\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{see also\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{о|\") != -1:\n",
    "        multiPage= True\n",
    "    elif text_lower.find(\"{{список однофамильцев}}\") != -1:\n",
    "        multiPage= True\n",
    "    categories = extract_cat(text)\n",
    "    \n",
    "    meaningPage = False\n",
    "    if (\"значения\" in title.lower()) or (\"значение\" in title.lower()):\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{неоднозначность') != -1:\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{многозначность') != -1:\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{disambig') != -1:\n",
    "        meaningPage = True\n",
    "    # redirects = extract_redirects(text)\n",
    "    links =[]\n",
    "    if not meaningPage:\n",
    "        links = extract_links(text)\n",
    "    else:\n",
    "        links = extract_first_links(text)\n",
    "    first_sentense = \"\"\n",
    "    if not redirect_page:\n",
    "        ext = Extractor(id,revid,\"\",title,page)\n",
    "        text_temp = \"\\n\".join(ext.clean_text(text))\n",
    "        first_sentense = extract_clear_text(text_temp.split(\".\")[0])\n",
    "        \n",
    "    if len(redirect_page) > 0:\n",
    "        if redirect_page not in dictRedirect:\n",
    "            dictRedirect[redirect_page] = []\n",
    "            dictPageRedirect[redirect_page] = []\n",
    "        dictPageRedirect[redirect_page].append(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect,first_sentense))\n",
    "        dictRedirect[redirect_page].append(title)\n",
    "        redirectcount +=1 \n",
    "    pages.append(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect,first_sentense))\n",
    "    # json.dump(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect),file,cls=PageEncoder,ensure_ascii=False)\n",
    "    # file.write(\"\\n\")\n",
    "#     if title == \"Abbath\" or title == \"Аббате\":\n",
    "#         print(text, file = log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c921fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение  информации о страницах\n",
    "# file = open(\"D:\\\\lbase_data\\\\ctxw.txt\", \"wb\")\n",
    "# pickle.dump(pages, file=file)\n",
    "# file.close()\n",
    "# file = open(\"D:\\\\lbase_data\\\\drp.txt\", \"wb\")\n",
    "# pickle.dump(dictPageRedirect, file=file)\n",
    "# file.close()\n",
    "# file = open(\"D:\\\\lbase_data\\\\dr.txt\", \"wb\")\n",
    "# pickle.dump(dictRedirect, file=file)\n",
    "# file.close()\n",
    "# input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7f51aa-7781-4cc8-a447-3040f44e857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\ctxw.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "pages = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\dr.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictRedirect = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\drp.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictPageRedirect = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06aa9261-4fcb-4ee2-8a1e-66a0dca64bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(pages):\n",
    "    if elem.title == \"Компьютерный язык\":\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381277bd-303c-4488-bed5-dd5fe8ae11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['язык',\n",
       " 'компьютерная техника',\n",
       " 'язык программирования',\n",
       " 'программист',\n",
       " 'компьютер',\n",
       " 'Компьютерный сленг',\n",
       " 'Язык программирования',\n",
       " 'Сценарный язык',\n",
       " 'Псевдоестественный язык',\n",
       " 'Информационный язык',\n",
       " 'Язык разметки',\n",
       " 'Язык спецификаций',\n",
       " 'Каскадные таблицы стилей',\n",
       " 'Язык описания аппаратуры',\n",
       " 'Протокол обмена',\n",
       " 'сетевой протокол',\n",
       " 'Компьютерный сленг']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1340].links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67727d62-1a2b-45bb-94b5-62e4b15d465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WikiSynset():\n",
    "    def __init__(self, page:Page):\n",
    "        self.page = page\n",
    "        self.synset = [page]\n",
    "\n",
    "    def append(self, redirect_title:Page):\n",
    "        self.synset.append(redirect_title)\n",
    "\n",
    "\n",
    "def includeTitleInWn(all_senses, title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_senses:\n",
    "        return True\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_senses:\n",
    "            return True\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word)\n",
    "                for word in text[0].split()])\n",
    "    if lemmatized.upper() in all_senses:\n",
    "        return True\n",
    "    if \"Ё\" in title:\n",
    "        return includeTitleInWn(all_senses, title.replace(\"Ё\",\"Е\"))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119f3970-e227-4156-8bf4-2c222525b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988017\n",
      "988017\n"
     ]
    }
   ],
   "source": [
    "print(len(dictPageRedirect))\n",
    "print(len(dictRedirect))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a387562-36f6-450c-bc9e-4327cd9a233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4432390/4432390 [00:03<00:00, 1349462.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 4432390/4432390 [00:01<00:00, 2844924.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 4432391/4432391 [04:57<00:00, 14922.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki = []\n",
    "meaningPageCounter = 0\n",
    "multiPageCounter = 0\n",
    "includeWS = 0\n",
    "includeTitle = 0\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "all_senses = set([' '.join([get_normal_form(w).upper() for w in s.lemma.split()]) for s in wn.senses])\n",
    "hashDict = {}\n",
    "for index in tqdm(range(len(pages)-1)):\n",
    "    hashDict[pages[index].title.lower()] = index\n",
    "#пройтись по всем значения со страницы-значения и всем значения поставить мульти\n",
    "i = 0\n",
    "for index in tqdm(range(len(pages)-1)):\n",
    "    if pages[index].meaningPage:\n",
    "        for link in pages[index].links:\n",
    "            if link.lower() in hashDict:\n",
    "                pages[hashDict[link.lower()]].multiPage = True\n",
    "                i += 1\n",
    "print(i)\n",
    "for page in tqdm(pages):\n",
    "    if page.redirect:\n",
    "        if includeTitleInWn(all_senses, page.title):\n",
    "            includeTitle += 1\n",
    "        continue\n",
    "    wikiSyn = WikiSynset(page)\n",
    "    if page.title in dictPageRedirect:\n",
    "        for redirect in dictPageRedirect[page.title]:\n",
    "            wikiSyn.append(redirect)\n",
    "    if page.meaningPage:\n",
    "        meaningPageCounter += 1\n",
    "    if page.multiPage:\n",
    "        multiPageCounter += 1\n",
    "    wiki.append(wikiSyn)\n",
    "    if includeTitleInWn(all_senses, page.title):\n",
    "        includeTitle += 1\n",
    "    # else:\n",
    "    #     print(page.title)\n",
    "page = 0\n",
    "print(multiPageCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ab8c90-e5f7-4d47-bf82-1633118ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# includeWS = 0\n",
    "# for wikisyn in tqdm(wiki):\n",
    "#     if includeTitleInWn(all_senses, str(wikisyn.page.title)) and (not wikisyn.page.multiPage) and (not wikisyn.page.meaningPage):\n",
    "#             includeWS += 1\n",
    "#     else:\n",
    "#         for synset in wikisyn.synset:\n",
    "#             if (includeTitleInWn(all_senses, str(synset.title)))and(not synset.multiPage) and (not synset.meaningPage):\n",
    "#                 includeWS += 1\n",
    "#                 break\n",
    "# print(\"count title:\", i)\n",
    "# print(\"count redirect\", redirectcount)\n",
    "# print(\"meaningPage count\", meaningPageCounter)\n",
    "# print(\"multiPage count\", multiPageCounter)\n",
    "# print(\"WikiSynset count\", len(wiki))\n",
    "# print(\"include title in wn count\", includeTitle)\n",
    "# print(\"include solo wikisyn in wn count\", includeWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cdfd79e-dc47-435d-b684-03c758b7732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display:\n",
    "    id:int\n",
    "    revid:int\n",
    "    title:str\n",
    "    lemma:str\n",
    "    wordId:int\n",
    "    ctxW:set\n",
    "class displayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, display):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9700189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxW(links, categories):\n",
    "    ctx = set()\n",
    "    for link in links:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in link.split()]))\n",
    "    for elem in categories:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in elem.split()]))\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff895bd4-c4bf-474d-8859-fb57b76a6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unambiguousDisplay(title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if len(wn.get_synsets(title)) == 1:\n",
    "        return [True, title]\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if len(wn.get_synsets(text[0])) == 1:\n",
    "            return [True, text[0]]\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word)\n",
    "                for word in text[0].split()])\n",
    "    if len(wn.get_synsets(lemmatized.upper())) == 1:\n",
    "        return [True, lemmatized.upper()]\n",
    "    if \"Ё\" in title:\n",
    "        return unambiguousDisplay(title.replace(\"Ё\",\"Е\"))\n",
    "    return [False, \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c6058a-bbaf-42d5-a9aa-82cf32c72f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #составляем контекст для слова из wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str\n",
    "class WnCtxEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, WnCtx):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "def getKeyForId(key):\n",
    "    if len(wn.get_senses(key)) > 0:\n",
    "        # print(1)\n",
    "        return wn.get_senses(key)[0].id\n",
    "    if \"(\" in key:\n",
    "        text = my_split(key).split(\",\")\n",
    "        if  len(wn.get_senses(text[0])) > 0:\n",
    "            # print(2)\n",
    "            return wn.get_senses(text[0])[0].id\n",
    "    text = my_split(key).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if len(wn.get_senses(lemmatized)) > 0:\n",
    "        # print(3)\n",
    "        return wn.get_senses(lemmatized)[0].id\n",
    "    if \"ё\" in key:\n",
    "        return getKeyForId(key.replace(\"ё\",\"у\"))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3855035c-6a0b-4ccc-93db-8f3b2e37032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ИСКУССТВЕННЫЙ ЯЗЫК', 'ЯЗЫК ПРОГРАММИРОВАНИЕ', 'ЭСПЕРАНТО', 'ЯЗЫК ПРОГРАММИРОВАНИЯ', 'КОМПЬЮТЕРНЫЙ ЯЗЫК'}\n"
     ]
    }
   ],
   "source": [
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "print(extractCtxS(wn, 'КОМПЬЮТЕРНЫЙ ЯЗЫК'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604a34b2-59cc-413d-94e6-edc15d94e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 4/94710 [00:00<4:23:47,  5.98it/s]Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1696, in _fetchall_impl\n",
      "    return list(self.iterator)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 147, in chunks\n",
      "    fetch = cursor._raw_all_rows()\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 392, in _raw_all_rows\n",
      "    rows = self._fetchall_impl()\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 1804, in _fetchall_impl\n",
      "    return self.cursor_strategy.fetchall(self, self.cursor)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 981, in fetchall\n",
      "    self.handle_exception(result, dbapi_cursor, e)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 941, in handle_exception\n",
      "    result.connection._handle_dbapi_exception(\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2047, in _handle_dbapi_exception\n",
      "    util.raise_(exc_info[1], with_traceback=exc_info[2])\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\", line 208, in raise_\n",
      "    raise exception\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 977, in fetchall\n",
      "    rows = dbapi_cursor.fetchall()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1914, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "sqlite3.ProgrammingError: Cannot operate on a closed database.\n",
      "  0%|                                                                              | 4/94710 [00:00<5:21:45,  4.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5672\\3349740056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtext_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lemma\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mctx_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractCtxS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mctx_s\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5672\\3181374621.py\u001b[0m in \u001b[0;36mextractCtxS\u001b[1;34m(wn, lemma)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#synonymy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msense\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_synsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msynonymy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msenses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mctx_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynonymy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#Hypernymy/Hyponymy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    479\u001b[0m                     \u001b[0mreplace_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                 )\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, state, dict_, passive)\u001b[0m\n\u001b[0;32m    939\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mPASSIVE_NO_RESULT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_loader_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mPASSIVE_NO_RESULT\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNO_VALUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36m_fire_loader_callables\u001b[1;34m(self, state, key, passive)\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mATTR_EMPTY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\strategies.py\u001b[0m in \u001b[0;36m_load_for_state\u001b[1;34m(self, state, passive, loadopt, extra_criteria)\u001b[0m\n\u001b[0;32m    909\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPASSIVE_NO_RESULT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         return self._emit_lazyload(\n\u001b[0m\u001b[0;32m    912\u001b[0m             \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\strategies.py\u001b[0m in \u001b[0;36m_emit_lazyload\u001b[1;34m(self, session, state, primary_key_identity, passive, loadopt, extra_criteria)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         )\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muselist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \"\"\"\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_allrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_allrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mmake_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmake_row\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mmade_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_real_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchmany_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1696\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1697\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36mchunks\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mfetch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_all_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msingle_entity\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_raw_all_rows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_all_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mmake_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchmany_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mhandle_exception\u001b[1;34m(self, result, dbapi_cursor, err)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhandle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         result.connection._handle_dbapi_exception(\n\u001b[0m\u001b[0;32m    942\u001b[0m             \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                 )\n\u001b[0;32m   2046\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#read word in wn\n",
    "mydoc = minidom.parse('D:\\\\lbase_data\\\\senses.N.xml')\n",
    "items = mydoc.getElementsByTagName('sense')\n",
    "countWn = 0\n",
    "dictWn = {}\n",
    "for elem in tqdm(items):\n",
    "    countWn +=1\n",
    "    text = elem.attributes['name'].value\n",
    "    text_id = elem.attributes[\"id\"].value\n",
    "    lemma = elem.attributes[\"lemma\"].value\n",
    "    ctx_s = extractCtxS(wn, lemma)\n",
    "    ctx = set()\n",
    "    for elem in ctx_s:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in elem.split()]))\n",
    "    dictWn[text_id] = WnCtx(text_id, ctx, lemma, text)\n",
    "print(len(dictWn), countWn)\n",
    "print(wn.get_synsets(\"замок\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e848d55-82d4-44f1-a546-ce50c419be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"D:\\\\lbase_data\\\\ctxS.txt\", \"wb\")\n",
    "# pickle.dump(dictWn, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753dc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb09a45b-9b8a-4714-a488-11b72deb9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WnCtx(id='130542-N-196389', ctx={'выверить', 'тщательно проверить', 'модерация контент', 'вычитка', 'вычитка текст', 'вычитывание', 'корректурный правка', 'редактирование текст'}, lemmaInWn='ВЫЧИТКА', name='ВЫЧИТКА')\n"
     ]
    }
   ],
   "source": [
    "# print(dictRedirect[\"Аббат\"])\n",
    "print(dictWn[\"130542-N-196389\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd212af-0a94-4204-915f-ff1cc3519368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLemmaForSynsets(title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    ch = \" \" \n",
    "    if len(wn.get_synsets(title))>0:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if len(wn.get_synsets(text[0]))>0:\n",
    "            if text[0][len(text[0]) - 1] == \" \":\n",
    "                text[0] = text[0].rstrip(ch)\n",
    "            return text[0]\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if len(wn.get_synsets(lemmatized))>0:\n",
    "        if lemmatized[len(lemmatized) - 1] == \" \":\n",
    "            lemmatized = lemmatized.rstrip(ch)\n",
    "        return lemmatized\n",
    "    if \"Ё\" in title:\n",
    "        return getLemmaForSynsets(title.replace(\"Ё\",\"Е\"))\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddcaf586-21ee-460b-a402-6d8334a882af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1826284/1826284 [13:20<00:00, 2281.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dictDisplay = {} # словарь отображений\n",
    "new_wiki = [] #будут все викисинсеты, кроме однозначных\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "for wikisyn in tqdm(wiki):\n",
    "    if len(wikisyn.synset) != 1:\n",
    "        new_wiki.append(wikisyn)\n",
    "        continue\n",
    "    one = unambiguousDisplay(wikisyn.page.title)\n",
    "    if  (not wikisyn.page.meaningPage) and (not wikisyn.page.multiPage) and (getKeyForId(wikisyn.page.title) in dictWn) and one[0] :\n",
    "        #id\n",
    "        temp = wn.get_synsets(one[1])[0].id\n",
    "        dictDisplay[wikisyn.page.title] = (display(wikisyn.page.id,wikisyn.page.revid,wikisyn.page.title,one[1], temp,\n",
    "                                                  extractCtxW(wikisyn.page.links, wikisyn.page.categories)))\n",
    "    else:\n",
    "        new_wiki.append(wikisyn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7088c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay  и new_wiki\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictDisplay1.txt\", \"wb\")\n",
    "# pickle.dump(dictDisplay, file=file)\n",
    "# file.close()\n",
    "# file = open(\"D:\\\\lbase_data\\\\new_wiki.txt\", \"wb\")\n",
    "# pickle.dump(new_wiki, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f6d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay1.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\new_wiki.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "new_wiki = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03b3f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     запись резулаов первого этапа в удобночиаемый вид\n",
    "file = open(\"D:\\\\lbase_data\\\\unambiguousDisplay.txt\", \"w\")\n",
    "for key in dictDisplay:\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" revid: \" + str(dictDisplay[key].revid) + \" title: \"+str(dictDisplay[key].title)  + \" lemma: \"  + str(dictDisplay[key].lemma) + \" wordId:\" + str(dictDisplay[key].wordId)+\"\\n\"\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06b7501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "031327db-d7fb-4d12-a4c6-59919b1a02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1823875/1823875 [51:10<00:00, 593.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#чуть раньше собираем кандидатов, для более точной\n",
    "#проверки многозначности страниц\n",
    "dictLemmaInIndex = {}\n",
    "for index in tqdm(range(len(new_wiki))):\n",
    "    lemma = getLemmaForSynsets(new_wiki[index].page.title)\n",
    "    if lemma != \"\":\n",
    "        if not lemma in dictLemmaInIndex:\n",
    "            dictLemmaInIndex[lemma] = []\n",
    "        dictLemmaInIndex[lemma].append(index)\n",
    "    else:\n",
    "        for elem in new_wiki[index].synset:\n",
    "            lemma1 = getLemmaForSynsets(elem.title)\n",
    "            if lemma1 != \"\":\n",
    "                if not lemma1 in dictLemmaInIndex:\n",
    "                    dictLemmaInIndex[lemma1] = []\n",
    "                dictLemmaInIndex[lemma1].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6a4fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictLemmaInIndex\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex.txt\", \"wb\")\n",
    "# pickle.dump(dictLemmaInIndex, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da947d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictLemmaInIndex = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3fe3a03-0ebe-4603-bd81-dde335be4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1823875/1823875 [00:01<00:00, 965483.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16957\n"
     ]
    }
   ],
   "source": [
    "#дополнительная проверка на многозначность\n",
    "dictTitleInIndex = {}\n",
    "for index in tqdm(range(len(new_wiki))):\n",
    "    dictTitleInIndex[new_wiki[index].page.title] = index\n",
    "tempCount = 0\n",
    "for key in dictLemmaInIndex:\n",
    "    for elem in dictLemmaInIndex[key]:\n",
    "        if not new_wiki[elem].page.multiPage:\n",
    "            if len(dictLemmaInIndex[key])>1:\n",
    "                new_wiki[dictTitleInIndex[new_wiki[elem].page.title]].page.multiPage = True\n",
    "                tempCount +=1\n",
    "print(tempCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "678ba80f-5329-407f-9a1c-03966e28ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823875\n",
      "2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████                                                 | 567121/1823875 [08:52<17:49, 1174.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аббат (музыкант)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1823875/1823875 [25:30<00:00, 1191.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10037 7628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def helpForCheck(d, title):\n",
    "    d = d.upper()\n",
    "    d = d.replace(\"—\", \"-\")\n",
    "    d = d.replace(\",\", \"\")\n",
    "    if d == title:\n",
    "        return True\n",
    "    if \"(\" in d:\n",
    "        text = my_split(d).split(\",\")\n",
    "        if title == text[0]:\n",
    "            return True\n",
    "    text = my_split(d).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if lemmatized == title:\n",
    "        return True\n",
    "    if \"Ё\" in d:\n",
    "        return helpForCheck(d.replace(\"Ё\",\"Е\"), d)\n",
    "    return False\n",
    "def check(wn, lemma, d):\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        if helpForCheck(d, sense.title):\n",
    "            return [True, sense]\n",
    "    return [False, \"ф\"]\n",
    "print(len(new_wiki))\n",
    "print(len(dictDisplay))\n",
    "wiki3 = [] # будут только многозначные\n",
    "# for w in tqdm(new_wiki):\n",
    "#     if w.page.title == \"Плебеи\":\n",
    "#         print(w.page.title, w.page.multiPage)\n",
    "#         print(unambiguousDisplay(w.page.title))\n",
    "#         print(getKeyForId(w.page.title))\n",
    "#         break\n",
    "countN = 0\n",
    "for w in tqdm(new_wiki):\n",
    "    flag = False\n",
    "    if not w.page.meaningPage and not w.page.multiPage:\n",
    "        for d in w.synset:\n",
    "            one = unambiguousDisplay(d.title)\n",
    "            #d.title in dictDisplay and\n",
    "            if  one[0] and \"N\" in wn.get_synsets(one[1])[0].id:\n",
    "                if w.page.title == \"Abbath\":\n",
    "                    print(d.title)\n",
    "                flag = True\n",
    "                countN += 1\n",
    "                idd = wn.get_synsets(one[1])[0].id\n",
    "                p = display(w.page.id,w.page.revid,w.page.title,one[1], idd,\n",
    "                            extractCtxW(w.page.links, w.page.categories))\n",
    "                dictDisplay[w.page.title]=p\n",
    "                break\n",
    "    if not flag:\n",
    "        wiki3.append(w)\n",
    "print(len(dictDisplay), countN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a851d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\secondpart.txt\", \"w\", encoding=\"utf-8\")\n",
    "for key in dictDisplay:\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" revid: \" + str(dictDisplay[key].revid) + \" title: \"+str(dictDisplay[key].title)  + \" lemma: \"  + str(dictDisplay[key].lemma) + \" wordId:\" + str(dictDisplay[key].wordId)+\"\\n\"\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay  и wiki3\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictDisplay2.txt\", \"wb\")\n",
    "# pickle.dump(dictDisplay, file=file)\n",
    "# file.close()\n",
    "# file = open(\"D:\\\\lbase_data\\\\wiki3.txt\", \"wb\")\n",
    "# pickle.dump(wiki3, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27de5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay2.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\wiki3.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "wiki3 = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeecf420-1e2b-491f-a527-8e9dc4a082f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1816247/1816247 [52:38<00:00, 574.99it/s]\n"
     ]
    }
   ],
   "source": [
    "#теперь собираем кандидатов только из оставшихся статей\n",
    "dictLemmaInIndex = {}\n",
    "for index in tqdm(range(len(wiki3))):\n",
    "    lemma = getLemmaForSynsets(wiki3[index].page.title)\n",
    "    if lemma != \"\":\n",
    "        if not lemma in dictLemmaInIndex:\n",
    "            dictLemmaInIndex[lemma] = []\n",
    "        dictLemmaInIndex[lemma].append(index)\n",
    "    else:\n",
    "        for elem in wiki3[index].synset:\n",
    "            lemma1 = getLemmaForSynsets(elem.title)\n",
    "            if lemma1 != \"\":\n",
    "                if not lemma1 in dictLemmaInIndex:\n",
    "                    dictLemmaInIndex[lemma1] = []\n",
    "                dictLemmaInIndex[lemma1].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9012962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictLemmaInIndex\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex1.txt\", \"wb\")\n",
    "# pickle.dump(dictLemmaInIndex, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ed58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex1.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictLemmaInIndex = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "932677fb-e093-43a5-8ca9-9e91d69df074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 114/1816247 [00:00<55:09, 548.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ВОЛГА 1\n",
      "АЙОВА 2\n",
      "АЙОВА 3\n",
      "АЙОВА 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1816247/1816247 [24:47<00:00, 1221.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#собираем кандидатов\n",
    "from collections import defaultdict\n",
    "dictSynsetId = defaultdict(list)\n",
    "countLinksAdd = 0\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "for index in tqdm(range(len(wiki3))):\n",
    "    w = wiki3[index]\n",
    "    if not w.page.meaningPage:\n",
    "        lemma = getLemmaForSynsets(w.page.title)\n",
    "        #если нашли лемму из ворднета и она существительное\n",
    "        if lemma != \"\" and getKeyForId(lemma) in dictWn:\n",
    "            for synset in wn.get_synsets(lemma):\n",
    "                dictSynsetId[synset.id].append(w)\n",
    "    else:\n",
    "        lemmaTitle = getLemmaForSynsets(w.page.title)\n",
    "        # if lemmaTitle.lower() == \"тихий океан\":\n",
    "        #     print(1111, index, w.page.links, w.page.title, w.page.meaningPage)\n",
    "        for link in w.page.links:\n",
    "            lemma = getLemmaForSynsets(link)\n",
    "            if lemma in dictLemmaInIndex:\n",
    "                #если нашли лемму многозначной статьи из ворднета и она существительное\n",
    "                if lemmaTitle != \"\" and getKeyForId(lemmaTitle) in dictWn:\n",
    "                    for synset in wn.get_synsets(lemmaTitle):\n",
    "                        countLinksAdd += 1\n",
    "                        if countLinksAdd < 5:\n",
    "                            print(lemma,countLinksAdd)\n",
    "                        for indexElem in dictLemmaInIndex[lemma]:\n",
    "                            dictSynsetId[synset.id].append(wiki3[indexElem])\n",
    "print(countLinksAdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e380c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictSynsetId\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictSynsetId.txt\", \"wb\")\n",
    "# pickle.dump(dictSynsetId, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd1bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictSynsetId.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictSynsetId = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be0a4857-233a-4b36-bf18-e5ce6254d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20435/20435 [00:01<00:00, 14441.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20435/20435 [00:00<00:00, 73323.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#возможны повторения среди кандидатов, уадлим их\n",
    "def count1(value, array):\n",
    "    count = 0\n",
    "    for elem in array:\n",
    "        if elem.page.id == value.page.id:\n",
    "            count += 1\n",
    "    return count\n",
    "for key in tqdm(dictSynsetId):\n",
    "    tempList = []\n",
    "    for elem in dictSynsetId[key]:\n",
    "        if count1(elem, tempList) < 1 and not elem.page.meaningPage:\n",
    "            tempList.append(elem)\n",
    "    dictSynsetId[key] = tempList\n",
    "#sort candidates\n",
    "sorted_tuple = sorted(dictSynsetId.items(), key=lambda x: x[0])\n",
    "sortCandidates = dict(sorted_tuple)\n",
    "dictIdTitle = {}\n",
    "for synset in wn.synsets:\n",
    "    dictIdTitle[synset.id] = synset.title\n",
    "#запись в файл претендентов\n",
    "candidates = open(\"D:\\\\lbase_data\\\\candidates.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for key in tqdm(sortCandidates):\n",
    "    s = \"id synset = \"+str(key) + \" synset title = \" + dictIdTitle[key]\n",
    "    for elem in sortCandidates[key]:\n",
    "        s += \" title = \" + elem.page.title +\" id = \" + elem.page.id + \" \"\n",
    "    print(s, file = candidates)\n",
    "candidates.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d0debe-7d09-466b-b2ba-a3e0ddd0cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(ctxS, ctxW):\n",
    "    return len(ctxS.intersection(ctxW))\n",
    "def get_key(key, dictWn):\n",
    "    key = key.lower()\n",
    "    key = key.replace(\"—\", \"-\")\n",
    "    key = key.replace(\",\", \"\")\n",
    "    id = getKeyForId(key)\n",
    "    if id in dictWn:\n",
    "        # print(1)\n",
    "        return id\n",
    "    if \"(\" in key:\n",
    "        id = wn.get_senses(text[0])[0].id\n",
    "        text = my_split(key).split(\",\")\n",
    "        if  id in dictWn:\n",
    "            # print(2)\n",
    "            return id\n",
    "    text = my_split(key).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    id = wn.get_senses(lemmatized)[0].id\n",
    "    if id in dictWn:\n",
    "        # print(3)\n",
    "        return id\n",
    "    if \"ё\" in key:\n",
    "        return get_key(key.replace(\"ё\",\"у\"), dictWn)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b2175a-4b6c-4419-bd9b-736feb980eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model_url, max_seq_length):\n",
    "    labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "    # Define input.\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"segment_ids\") \n",
    "    # LaBSE layer.\n",
    "    pooled_output,_ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "    # The embedding is l2 normalized.\n",
    "    pooled_output = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "      # Define model.\n",
    "    return tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids],outputs=pooled_output), labse_layer\n",
    "\n",
    "max_seq_length = 64\n",
    "labse_model, labse_layer = get_model(model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "    input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
    "    for input_string in input_strings:\n",
    "    # Tokenize input.\n",
    "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        sequence_length = min(len(input_ids), max_seq_length)\n",
    "\n",
    "\n",
    "    # Padding or truncation.\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "        input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    segment_ids_all.append([0] * max_seq_length) \n",
    "    return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n",
    "\n",
    "def encode(input_text):\n",
    "    input_ids, input_mask, segment_ids = create_input(\n",
    "      input_text, tokenizer, max_seq_length)\n",
    "    return labse_model([input_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79190696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosin_distance(word, sentense):\n",
    "    word_embeding = encode([word])[0].numpy()\n",
    "    sentense_embeding = encode([sentense])[0].numpy()\n",
    "    return np.dot(word_embeding, sentense_embeding) / (sum(sentense_embeding ** 2) * sum(word_embeding ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e4b25-500a-4678-a3e8-518530d36d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████                                                         | 4506/20435 [1:30:10<6:28:54,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#3 этап алгоритма\n",
    "badlemma = []\n",
    "baddenominator = []\n",
    "badmaxP = []\n",
    "badsynsetlemma = []\n",
    "badidWn = []\n",
    "print(len(dictDisplay))\n",
    "dictSortCandidates = {}\n",
    "for key in tqdm(sortCandidates):\n",
    "    if len(sortCandidates[key]) == 1:\n",
    "            w = sortCandidates[key][0]\n",
    "            p = display(w.page.id,w.page.revid,w.page.title,dictIdTitle[key], key,extractCtxW(w.page.links, w.page.categories))\n",
    "            dictDisplay[w.page.title]=p\n",
    "            dictSortCandidates[key] = [(sortCandidates[key][0], 1)]\n",
    "    else:\n",
    "        maxP = -1\n",
    "        maxagrument = 0\n",
    "        lemmaSynset = getLemmaForSynsets(dictIdTitle[key])\n",
    "        dictSortCandidates[key] = []\n",
    "        if lemmaSynset != \"\":\n",
    "            idWn = wn.get_senses(lemmaSynset)[0].id\n",
    "            if \"N\" in idWn: #почему-то иногда для синсета существительного сенс не существительное\n",
    "                for elem in sortCandidates[key]:\n",
    "                    ctxw = extractCtxW(elem.page.links, elem.page.categories)\n",
    "                    lemma = getLemmaForSynsets(elem.page.title)\n",
    "                    if lemma != \"\":\n",
    "                        numerator = score(dictWn[idWn].ctx, ctxw)\n",
    "                        denominator = 0\n",
    "                        for item in sortCandidates[key]:\n",
    "                            addctxw = extractCtxW(item.page.links, item.page.categories)\n",
    "                            denominator +=score(dictWn[idWn].ctx, addctxw)\n",
    "                    else:\n",
    "                        badlemma.append(elem.page.title)\n",
    "                    temp_distance = cosin_distance(lemmaSynset, elem.page.first_sentence)\n",
    "                    if denominator != 0:\n",
    "                        temp_p = numerator / denominator +  temp_distance \n",
    "                        dictSortCandidates[key].append((elem, temp_p))                           \n",
    "                        if  temp_p > maxP:\n",
    "                            maxP = temp_p\n",
    "                            maxagrument = elem\n",
    "                    else:\n",
    "                        if  temp_distance > maxP:\n",
    "                            maxP = temp_distance\n",
    "                            maxagrument = elem\n",
    "                        baddenominator.append(elem.page.title)\n",
    "                        dictSortCandidates[key].append((elem, temp_distance))\n",
    "            else:\n",
    "                badidWn.append(wn.get_senses(lemmaSynset)[0].id)\n",
    "        else:\n",
    "            badsynsetlemma.append(dictIdTitle[key])\n",
    "        if maxP != - 1:\n",
    "            w = maxagrument\n",
    "            p = display(w.page.id,w.page.revid,w.page.title,dictIdTitle[key], key,\n",
    "                        extractCtxW(w.page.links, w.page.categories))\n",
    "            dictDisplay[w.page.title]=p\n",
    "        else:\n",
    "            badmaxP.append(key)\n",
    "print(\"len(dictDisplay)\",len(dictDisplay)) \n",
    "print(\"len(badlemma)\",len(badlemma))\n",
    "print(\"len(baddenominator)\",len(baddenominator))\n",
    "print(\"len(badmaxP)\",len(badmaxP))\n",
    "print(\"len(badsynsetlemma)\",len(badsynsetlemma))\n",
    "print(\"len(badidWn)\",len(badidWn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6104a-607a-4f1b-97b5-26b73f333b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dictSortCandidates:\n",
    "    temp = sorted(dictSortCandidates[key],key=lambda x: x[1], reverse = True)\n",
    "    dictSortCandidates[key] = temp\n",
    "candidates = open(\"D:\\\\lbase_data\\\\sortcandidates.txt\", \"w\", encoding=\"utf-8\")\n",
    "candidates1 = open(\"D:\\\\lbase_data\\\\sortcandidates1000.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for key in tqdm(dictSortCandidates):\n",
    "    s = \"id synset: \"+str(key) + \"\\tsynset title: \" + dictIdTitle[key] + \"\\n\" \n",
    "    lemmaSynset = getLemmaForSynsets(dictIdTitle[key])\n",
    "    if lemmaSynset != \"\" :\n",
    "        idWn = wn.get_senses(lemmaSynset)[0].id\n",
    "        if \"N\" in idWn:\n",
    "            s += \"Ctx synset: \" + \",\".join(list(dictWn[idWn].ctx)) + \"\\n\"\n",
    "        else:\n",
    "            s += \"id for N is A. lemma: \" + lemmaSynset + \"\\n\"\n",
    "    else:\n",
    "        s += \"lemmasynset not found\\n\" \n",
    "    for elem in dictSortCandidates[key]:\n",
    "        s += \"title: \" + elem[0].page.title +\" id = \" + elem[0].page.id + \" \" + str(elem[1]) + \"\\n\"\n",
    "        s += \"ctx article:\" +\",\".join(list(extractCtxW(elem[0].page.links,elem[0].page.categories))) +\"\\n\"\n",
    "    s += \"\\n\"\n",
    "    print(s, file = candidates)\n",
    "    if i < 1000:\n",
    "        print(s, file = candidates1) \n",
    "    i += 1\n",
    "candidates.close()\n",
    "candidates1.close()\n",
    "# file = open(\"thirdpart.txt\",\"w\", encoding=\"utf-8\")\n",
    "# for key in tqdm(dictDisplay):\n",
    "#     temp = \"id:  \"+str(dictDisplay[key].id) + \" revid: \" + str(dictDisplay[key].revid) + \" title: \"+str(dictDisplay[key].title) +'\\n'\n",
    "#     file.write(temp)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb572f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay\n",
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay3.txt\", \"wb\")\n",
    "pickle.dump(dictDisplay, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc02f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay3.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\thirdpart.txt\",\"w\", encoding=\"utf-8\")\n",
    "for key in tqdm(dictDisplay):\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" wordId: \" + str(dictDisplay[key].wordId) + \" title: \"+str(dictDisplay[key].title) +\" lemma: \"+str(dictDisplay[key].lemma) + '\\n'\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0f596-8063-492a-ba25-f32490084ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем отображения  связанные содним wordId\n",
    "dict_wordId_in_display_and_key = defaultdict(list)\n",
    "for key, value in dictDisplay.items():\n",
    "    dict_wordId_in_display_and_key[value.wordId].append((key,value))\n",
    "i  = 0\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        i+=len(value)\n",
    "i, len(dict_wordId_in_display_and_key), len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75628e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#удалим из dictDisplay повторяющиеся wordId\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        for item in value:\n",
    "            del dictDisplay[item[0]]\n",
    "#удалим из dict_wordId_in_display_and_key однозначные отображенния\n",
    "dict_wordId_in_display_and_key_new = defaultdict(list)\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        dict_wordId_in_display_and_key_new[key] = value\n",
    "len(dict_wordId_in_display_and_key_new), len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6231ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denominator = 0 for all candidates for key:1412-N synset is ТВОРЧЕСКИЙ РАБОТНИК\n",
      "denominator = 0 for all candidates for key:2323-N synset is АВТОМОБИЛЬНЫЕ ПЕРЕВОЗКИ\n",
      "lemmaSynset for key 126491-N is none, synset is КОНДЕНСАТ, ПРОДУКТ КОНДЕНСАЦИИ\n",
      "lemma for key 9018-N is not noun, lemmaSynset is ПОЖАРНЫЙ for sysnet ПОЖАРНЫЙ\n",
      "denominator = 0 for all candidates for key:125677-N synset is АВТОРИТЕТНЫЙ ЧЕЛОВЕК\n",
      "denominator = 0 for all candidates for key:155447-N synset is ЭЛЛИПТИЧЕСКИЙ ТРЕНАЖЕР\n",
      "denominator = 0 for all candidates for key:5417-N synset is ПАЦИЕНТ\n",
      "denominator = 0 for all candidates for key:117668-N synset is КАНАТ\n",
      "denominator = 0 for all candidates for key:136939-N synset is КОПРА КОКОСА\n",
      "denominator = 0 for all candidates for key:9472-N synset is ЛЕПРОЗОРИЙ\n",
      "denominator = 0 for all candidates for key:125059-N synset is АНТИТЕЗИС\n",
      "denominator = 0 for all candidates for key:110714-N synset is НАЛИВКА\n",
      "denominator = 0 for all candidates for key:100319-N synset is КАДЖАРАН\n",
      "lemmaSynset for key 135636-N is none, synset is ЦЕЙТНОТ, НЕДОСТАТОК ВРЕМЕНИ\n",
      "denominator = 0 for all candidates for key:140087-N synset is ДУХОВНЫЕ ЦЕННОСТИ\n",
      "denominator = 0 for all candidates for key:102762-N synset is КРОЛЕВЕЦ\n",
      "denominator = 0 for all candidates for key:153068-N synset is СМОТРИНЫ НЕВЕСТЫ\n",
      "lemmaSynset for key 130720-N is none, synset is ГРАФОЛОГИЯ, ПОЧЕРКОВЕДЕНИЕ\n",
      "lemmaSynset for key 106446-N is none, synset is ОБСТОЯТЕЛЬСТВА, ОБСТАНОВКА, УСЛОВИЯ\n",
      "denominator = 0 for all candidates for key:4745-N synset is НАУЧНО-ТЕХНИЧЕСКИЙ ПРОГРЕСС\n",
      "lemmaSynset for key 113507-N is none, synset is ХРАПЕТЬ ВО СНЕ\n",
      "denominator = 0 for all candidates for key:154953-N synset is ТУРЕЦКИЙ ГОРОХ\n",
      "denominator = 0 for all candidates for key:100259-N synset is БИРА\n",
      "denominator = 0 for all candidates for key:109059-N synset is КРЕП\n",
      "lemmaSynset for key 120612-N is none, synset is ЛЕПИТЬ, ВЫЛЕПИТЬ\n",
      "lemma for key 119906-N is not noun, lemmaSynset is САМОПРОИЗВОЛЬНЫЙ for sysnet САМОПРОИЗВОЛЬНЫЙ\n",
      "denominator = 0 for all candidates for key:9026-N synset is СИДЕЛКА\n",
      "denominator = 0 for all candidates for key:127525-N synset is ПЕРЛАМУТР (ЦЕННОЕ ВЕЩЕСТВО)\n",
      "denominator = 0 for all candidates for key:134701-N synset is РЯБЬ В ГЛАЗАХ\n",
      "lemmaSynset for key 124609-N is none, synset is ССОРА, РАЗДОР\n",
      "denominator = 0 for all candidates for key:145335-N synset is РАДИОМОНИТОРИНГ\n",
      "denominator = 0 for all candidates for key:128633-N synset is ЦИФЕРБЛАТ\n",
      "lemmaSynset for key 110831-N is none, synset is ОТПИЛИТЬ, СПИЛИТЬ\n",
      "denominator = 0 for all candidates for key:109009-N synset is РУБИЛЬНИК\n",
      "lemma for key 121340-N is not noun, lemmaSynset is ЖАРОПРОЧНЫЙ for sysnet ЖАРОПРОЧНЫЙ\n",
      "denominator = 0 for all candidates for key:126786-N synset is СУДОСТРОИТЕЛЬ\n",
      "lemmaSynset for key 112832-N is none, synset is НЕБО, НЕБЕСНЫЙ СВОД\n",
      "lemmaSynset for key 114566-N is none, synset is ШПИК, СВИНОЕ САЛО\n",
      "denominator = 0 for all candidates for key:123400-N synset is ЭШАФОТ\n",
      "denominator = 0 for all candidates for key:104380-N synset is БАРБУДА\n",
      "denominator = 0 for all candidates for key:113338-N synset is ДРУГ (ПРИЯТЕЛЬ)\n",
      "denominator = 0 for all candidates for key:114266-N synset is ТАБАК (ПРОДУКТ)\n",
      "denominator = 0 for all candidates for key:6427-N synset is НЕПРИКОСНОВЕННОСТЬ ЛИЧНОСТИ\n",
      "denominator = 0 for all candidates for key:138554-N synset is ДОМ-МУЗЕЙ\n",
      "denominator = 0 for all candidates for key:107338-N synset is ХЛАДНОКРОВИЕ\n",
      "lemmaSynset for key 121026-N is none, synset is КАЛ, ИСПРАЖНЕНИЯ\n",
      "lemmaSynset for key 140320-N is none, synset is МАГАЗИН УЦЕНЕННЫХ ТОВАРОВ\n",
      "lemmaSynset for key 148577-N is none, synset is КУЗОВ МИНИВЭН\n",
      "denominator = 0 for all candidates for key:4255-N synset is ОХРАНА ВОД\n",
      "lemma for key 141729-N is not noun, lemmaSynset is заступить за линия for sysnet ЗАСТУПИТЬ ЗА ЛИНИЮ\n",
      "lemmaSynset for key 126702-N is none, synset is НИЧТОЖЕСТВО, НИЧТОЖНЫЙ ЧЕЛОВЕК\n",
      "denominator = 0 for all candidates for key:124136-N synset is ТРАНШ КРЕДИТА\n",
      "denominator = 0 for all candidates for key:164746-N synset is ПЕРЕСЛАВЦЫ\n",
      "denominator = 0 for all candidates for key:131939-N synset is КОЛОННАДА (АРХИТЕКТУРНОЕ СООРУЖЕНИЕ)\n",
      "denominator = 0 for all candidates for key:116023-N synset is ЗЕМЛЕСОСНЫЙ СНАРЯД\n",
      "denominator = 0 for all candidates for key:125529-N synset is ВАТАГА (ШУМНАЯ ТОЛПА)\n",
      "denominator = 0 for all candidates for key:107047-N synset is МИСКА\n",
      "lemmaSynset for key 115479-N is none, synset is УМ, ИНТЕЛЛЕКТ\n",
      "denominator = 0 for all candidates for key:116199-N synset is ПРОРУБЬ\n",
      "lemmaSynset for key 134182-N is none, synset is ПРОВИДЕНИЕ, ПРОМЫСЛ БОЖИЙ\n",
      "lemma for key 119067-N is not noun, lemmaSynset is ВОПЛОТИТЬ for sysnet ВОПЛОТИТЬ\n",
      "lemmaSynset for key 7380-N is none, synset is БИОРЕСУРСЫ\n",
      "denominator = 0 for all candidates for key:129684-N synset is ЯХТ-КЛУБ (СПОРТИВНАЯ ОРГАНИЗАЦИЯ)\n",
      "lemmaSynset for key 115440-N is none, synset is РАСПОЛОЖЕНИЕ, СИМПАТИЯ\n",
      "denominator = 0 for all candidates for key:2612-N synset is ПРЕДПРАЗДНИЧНЫЙ ДЕНЬ\n",
      "lemmaSynset for key 131681-N is none, synset is СТОЛП, МЭТР\n",
      "denominator = 0 for all candidates for key:114490-N synset is ХРОНОМЕТРАЖ\n",
      "denominator = 0 for all candidates for key:165477-N synset is СЧИТЫВАТЕЛЬ\n",
      "denominator = 0 for all candidates for key:115755-N synset is СЕЯНЕЦ\n",
      "denominator = 0 for all candidates for key:131426-N synset is ЦЕНТРАЛЬНЫЙ НАПАДАЮЩИЙ\n",
      "denominator = 0 for all candidates for key:109681-N synset is НЕЙРОН\n",
      "denominator = 0 for all candidates for key:108623-N synset is ЛОЗА (СТЕБЕЛЬ)\n",
      "denominator = 0 for all candidates for key:105674-N synset is РОГУН\n",
      "denominator = 0 for all candidates for key:112698-N synset is ПАРША\n",
      "denominator = 0 for all candidates for key:134583-N synset is МОРСКОЙ РИФ\n",
      "denominator = 0 for all candidates for key:115699-N synset is ТОМ (КНИГА)\n",
      "denominator = 0 for all candidates for key:144230-N synset is КАЛИНА (РАСТЕНИЕ)\n",
      "denominator = 0 for all candidates for key:113867-N synset is УСЕРДИЕ\n",
      "denominator = 0 for all candidates for key:135092-N synset is ЦЫГАНСКИЙ ТАБОР\n",
      "denominator = 0 for all candidates for key:120544-N synset is ШАЛОПАЙ\n",
      "denominator = 0 for all candidates for key:100942-N synset is СИСИМ\n",
      "lemmaSynset for key 164182-N is none, synset is КИНЕШЕМЦЫ\n",
      "denominator = 0 for all candidates for key:124704-N synset is ТОРЕЦ (ПОПЕРЕЧНАЯ ГРАНЬ)\n",
      "lemmaSynset for key 125931-N is none, synset is КРИКУН, ГОРЛОПАН\n",
      "denominator = 0 for all candidates for key:112693-N synset is КОНКУРЕНТ\n",
      "lemma for key 112461-N is not noun, lemmaSynset is ЛАЯТЬ for sysnet ЛАЯТЬ (ИЗДАВАТЬ ЗВУК)\n",
      "lemmaSynset for key 2518-N is none, synset is СОЦИАЛЬНАЯ НАПРЯЖЕННОСТЬ\n",
      "lemmaSynset for key 4350-N is none, synset is ЛИЦО БЕЗ ОПРЕДЕЛЕННОГО МЕСТА ЖИТЕЛЬСТВА\n",
      "lemmaSynset for key 8193-N is none, synset is АВАРИЙНОЕ СОСТОЯНИЕ\n",
      "denominator = 0 for all candidates for key:103696-N synset is ДЖУРА\n",
      "denominator = 0 for all candidates for key:107721-N synset is ЛОКОТЬ (СУСТАВ)\n",
      "denominator = 0 for all candidates for key:129492-N synset is ХОБОТ ЖИВОТНОГО\n",
      "lemmaSynset for key 125575-N is none, synset is ТРАНЖИРА, МОТ\n",
      "lemmaSynset for key 126265-N is none, synset is ТУМАК, УДАР КУЛАКОМ\n",
      "lemmaSynset for key 144297-N is none, synset is УПРАВЛЕНИЕ ФИНАНСОВЫМИ РЕСУРСАМИ\n",
      "denominator = 0 for all candidates for key:108962-N synset is ГОНГ\n",
      "lemma for key 110722-N is not noun, lemmaSynset is разлить по сосуд for sysnet РАЗЛИТЬ ПО СОСУДАМ\n",
      "denominator = 0 for all candidates for key:132114-N synset is ЛАКУНА В ТЕКСТЕ\n",
      "denominator = 0 for all candidates for key:107019-N synset is ТРЕТЬ\n",
      "denominator = 0 for all candidates for key:107939-N synset is ЛАЗ (ОТВЕРСТИЕ)\n",
      "lemmaSynset for key 107260-N is none, synset is ПРИХОТЬ, ПРИЧУДА, КАПРИЗ\n",
      "lemmaSynset for key 129759-N is none, synset is ВЕТОШЬ, ТРЯПКА ДЛЯ ПРОТИРКИ\n",
      "lemma for key 119491-N is not noun, lemmaSynset is ОТВЕРДЕТЬ for sysnet ОТВЕРДЕТЬ\n",
      "lemma for key 115225-N is not noun, lemmaSynset is СОСАТЬ for sysnet СОСАТЬ (ВТЯГИВАТЬ В РОТ)\n",
      "denominator = 0 for all candidates for key:109633-N synset is ТЛЯ\n",
      "lemmaSynset for key 126525-N is none, synset is КРИВЛЯКА, ЛОМАКА\n",
      "denominator = 0 for all candidates for key:136900-N synset is ТРУДОЛЮБИВЫЙ ЧЕЛОВЕК\n",
      "lemma for key 119550-N is not noun, lemmaSynset is НЕСПРАВЕДЛИВЫЙ for sysnet НЕСПРАВЕДЛИВЫЙ (ПРОТИВОРЕЧАЩИЙ СПРАВЕДЛИВОСТИ)\n",
      "denominator = 0 for all candidates for key:114951-N synset is ИРРИГАТОР\n",
      "denominator = 0 for all candidates for key:148445-N synset is ЧУЖАЯ СТРАНА\n",
      "lemmaSynset for key 125409-N is none, synset is БИОПОЛЕ, АУРА\n",
      "denominator = 0 for all candidates for key:102969-N synset is УТВА\n",
      "denominator = 0 for all candidates for key:164503-N synset is ВЫШНЕВОЛОЧАНЕ\n",
      "denominator = 0 for all candidates for key:7685-N synset is ИЗЫСКАТЕЛЬСКИЕ РАБОТЫ\n",
      "lemmaSynset for key 107458-N is none, synset is КРАСИТЬ, ПОКРЫВАТЬ КРАСКОЙ\n",
      "denominator = 0 for all candidates for key:102158-N synset is КОТРА\n",
      "denominator = 0 for all candidates for key:125161-N synset is БАРРАЖИРОВАНИЕ\n",
      "lemmaSynset for key 125849-N is none, synset is ГАМ, ГОМОН\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denominator = 0 for all candidates for key:112694-N synset is НАСЛАЖДЕНИЕ\n",
      "denominator = 0 for all candidates for key:879-N synset is ТРУДОВОЙ КОЛЛЕКТИВ\n",
      "denominator = 0 for all candidates for key:114181-N synset is ОСНОВАТЕЛЬ\n",
      "lemmaSynset for key 138237-N is none, synset is ЗАТРУДНЕНИЕ, ПРОБЛЕМА\n",
      "denominator = 0 for all candidates for key:129163-N synset is КАЗАНЦЫ\n",
      "denominator = 0 for all candidates for key:107524-N synset is ПОСЛЕДОВАТЕЛЬ\n",
      "lemmaSynset for key 135705-N is none, synset is ЭТИМОЛОГИЯ СЛОВА, ВЫРАЖЕНИЯ\n",
      "denominator = 0 for all candidates for key:101328-N synset is ЯГОТИН\n",
      "denominator = 0 for all candidates for key:991-N synset is ЛЕСОЗАГОТОВИТЕЛЬНЫЕ РАБОТЫ\n",
      "lemma for key 111100-N is not noun, lemmaSynset is ПЛАВИТЬ for sysnet ПЛАВИТЬ\n",
      "lemmaSynset for key 146997-N is none, synset is ПОДРУГА, ЛЮБИМАЯ ДЕВУШКА\n",
      "lemmaSynset for key 106968-N is none, synset is ВОСТОРГ, ВОСХИЩЕНИЕ\n",
      "lemmaSynset for key 6524-N is none, synset is ЗОНА ВООРУЖЕННОГО КОНФЛИКТА\n",
      "denominator = 0 for all candidates for key:115323-N synset is ПОХИТИТЕЛЬ\n",
      "denominator = 0 for all candidates for key:131999-N synset is КРАП (РАСЦВЕТКА)\n",
      "denominator = 0 for all candidates for key:155345-N synset is СПАЗМОЛИТИК\n",
      "lemmaSynset for key 108220-N is none, synset is ВЕРБА, ИВА ОСТРОЛИСТНАЯ\n",
      "lemma for key 119523-N is not noun, lemmaSynset is ДУШЕВНЫЙ for sysnet ДУШЕВНЫЙ (ИСКРЕННИЙ, ДРУЖЕЛЮБНЫЙ)\n",
      "denominator = 0 for all candidates for key:120685-N synset is МОДНИК\n",
      "denominator = 0 for all candidates for key:137366-N synset is ФЕНОМЕН (НЕОБЫЧНОЕ ЯВЛЕНИЕ)\n",
      "lemmaSynset for key 129174-N is none, synset is МОЛОКОСОС, СОПЛЯК\n",
      "lemmaSynset for key 128240-N is none, synset is ПРАВОВЕРНЫЙ, НАБОЖНЫЙ\n",
      "denominator = 0 for all candidates for key:113965-N synset is КОРМУШКА ДЛЯ ЖИВОТНЫХ\n",
      "lemmaSynset for key 125180-N is none, synset is БОДРСТВОВАТЬ, НЕ СПАТЬ\n",
      "denominator = 0 for all candidates for key:125961-N synset is ГРОМАДА (ОГРОМНЫЙ ПРЕДМЕТ)\n",
      "lemmaSynset for key 118499-N is none, synset is ВСПОМНИТЬ, ВОЗОБНОВИТЬ В ПАМЯТИ\n",
      "denominator = 0 for all candidates for key:141448-N synset is ГЕГЕМОНИЯ (ГОСПОДСТВО)\n",
      "denominator = 0 for all candidates for key:100569-N synset is ЦИПА\n",
      "lemmaSynset for key 128459-N is none, synset is ПУСТОСЛОВ, ПУСТОМЕЛЯ\n",
      "denominator = 0 for all candidates for key:107717-N synset is НОГА (НИЖНЯЯ КОНЕЧНОСТЬ)\n",
      "denominator = 0 for all candidates for key:109860-N synset is СЫПЬ НА КОЖЕ\n",
      "denominator = 0 for all candidates for key:100869-N synset is СЕРЕТ\n",
      "denominator = 0 for all candidates for key:144923-N synset is ПОЛИМЕТАЛЛИЧЕСКИЕ РУДЫ\n",
      "denominator = 0 for all candidates for key:118618-N synset is ЗАСТУПНИК\n",
      "denominator = 0 for all candidates for key:107269-N synset is КОЖУРА\n",
      "denominator = 0 for all candidates for key:1792-N synset is ДОЛГОВОЕ ОБЯЗАТЕЛЬСТВО\n",
      "denominator = 0 for all candidates for key:112673-N synset is КОБЫЛА\n",
      "denominator = 0 for all candidates for key:100941-N synset is КЕБЕЖ\n",
      "lemmaSynset for key 133968-N is none, synset is ПОДТЕКСТ, СКРЫТЫЙ СМЫСЛ\n",
      "denominator = 0 for all candidates for key:100435-N synset is СЮНЬ\n",
      "denominator = 0 for all candidates for key:4587-N synset is ЧИТАЛЬНЫЙ ЗАЛ\n",
      "denominator = 0 for all candidates for key:137527-N synset is ХИТ-ПАРАД\n",
      "denominator = 0 for all candidates for key:109427-N synset is РУЛЬ\n",
      "denominator = 0 for all candidates for key:2460-N synset is ИНДУСТРИАЛЬНОЕ ДОМОСТРОЕНИЕ\n",
      "lemmaSynset for key 115707-N is none, synset is ЩЕПКА, ЩЕПА\n",
      "denominator = 0 for all candidates for key:1555-N synset is ИНВЕСТИЦИОННАЯ ПОЛИТИКА\n",
      "denominator = 0 for all candidates for key:139217-N synset is НИЖЕГОРОДЦЫ\n",
      "lemmaSynset for key 132158-N is none, synset is ЛОЖКАРЬ, ЛОЖЕЧНИК\n",
      "lemmaSynset for key 5986-N is none, synset is БЕДНОСТЬ, НУЖДА\n",
      "denominator = 0 for all candidates for key:2859-N synset is КОПИРОВАЛЬНАЯ ТЕХНИКА\n",
      "denominator = 0 for all candidates for key:125930-N synset is ГОРБУШКА ХЛЕБА\n",
      "lemmaSynset for key 126360-N is none, synset is ИСКРА, ГОРЯЩАЯ ЧАСТИЦА\n",
      "denominator = 0 for all candidates for key:107365-N synset is ВИЗГ\n",
      "denominator = 0 for all candidates for key:125243-N synset is БЕЗДОРОЖЬЕ\n",
      "denominator = 0 for all candidates for key:109272-N synset is ШЛАНГ\n",
      "denominator = 0 for all candidates for key:115251-N synset is ИВНЯК\n",
      "denominator = 0 for all candidates for key:134306-N synset is СЕВЕРО-ЗАПАДНЫЙ ФЕДЕРАЛЬНЫЙ ОКРУГ\n",
      "lemmaSynset for key 4987-N is none, synset is КРАСИТЕЛЬ, КРАСЯЩИЙ АГЕНТ\n",
      "denominator = 0 for all candidates for key:110737-N synset is ЧАСТУШКА\n",
      "lemmaSynset for key 115212-N is none, synset is ПОЗОРИТЬ, ПОРОЧИТЬ\n",
      "lemmaSynset for key 117051-N is none, synset is ПЕРЕЧИТЬ, ПРОТИВОРЕЧИТЬ\n",
      "lemmaSynset for key 132689-N is none, synset is МНОГОЗНАЧНЫЙ, НЕОДНОЗНАЧНЫЙ\n",
      "denominator = 0 for all candidates for key:135897-N synset is ДОМЕН СЕТИ ИНТЕРНЕТ\n",
      "lemmaSynset for key 132693-N is none, synset is МОДЕЛИРОВАНИЕ ЯВЛЕНИЯ, ПРОЦЕССА\n",
      "lemmaSynset for key 106471-N is none, synset is НАМЕРЕНИЕ, ЗАМЫСЕЛ\n",
      "denominator = 0 for all candidates for key:7978-N synset is ГЕНЕРАЛЬНЫЙ СЕКРЕТАРЬ\n",
      "denominator = 0 for all candidates for key:4758-N synset is КАБОТАЖ\n",
      "denominator = 0 for all candidates for key:5805-N synset is СЕРДЕЧНО-СОСУДИСТАЯ СИСТЕМА\n",
      "denominator = 0 for all candidates for key:123617-N synset is КОМПЛЕКС НЕПОЛНОЦЕННОСТИ\n",
      "denominator = 0 for all candidates for key:107542-N synset is НАДПИСЬ (КОРОТКИЙ ТЕКСТ)\n",
      "lemmaSynset for key 115657-N is none, synset is ПЕРЕЕСТЬ, СЪЕСТЬ ЛИШНЕЕ\n",
      "denominator = 0 for all candidates for key:6738-N synset is НЕЗАМУЖНЯЯ ЖЕНЩИНА\n",
      "denominator = 0 for all candidates for key:138575-N synset is НАСТОЯЩИЙ МУЖЧИНА\n",
      "denominator = 0 for all candidates for key:139083-N synset is НЕДОСТОЙНОЕ ПОВЕДЕНИЕ\n",
      "denominator = 0 for all candidates for key:125267-N synset is БОРОДАВОЧНИК\n",
      "denominator = 0 for all candidates for key:109167-N synset is ПАНДА\n",
      "denominator = 0 for all candidates for key:111273-N synset is НЕГОДЯЙ\n",
      "lemma for key 118899-N is not noun, lemmaSynset is ПРИСТАТЬ for sysnet ПРИСТАТЬ (НАДОЕДАТЬ)\n",
      "denominator = 0 for all candidates for key:107830-N synset is МУРАВЬЕД\n",
      "denominator = 0 for all candidates for key:128367-N synset is ИНТРИГАН\n",
      "denominator = 0 for all candidates for key:126279-N synset is ЗЕВОТА\n",
      "denominator = 0 for all candidates for key:124562-N synset is РЮШ\n",
      "denominator = 0 for all candidates for key:128063-N synset is ХОРЕЙ\n",
      "lemma for key 119377-N is not noun, lemmaSynset is СИНХРОННЫЙ for sysnet СИНХРОННЫЙ (ОДНОВРЕМЕННЫЙ)\n",
      "lemmaSynset for key 3493-N is none, synset is ОБОРОТНЫЕ СРЕДСТВА\n",
      "denominator = 0 for all candidates for key:4673-N synset is ИНКАССО\n",
      "lemmaSynset for key 6351-N is none, synset is САДОВО-ОГОРОДНИЧЕСКОЕ ТОВАРИЩЕСТВО\n",
      "lemmaSynset for key 126117-N is none, synset is ИСПОВЕДНИК, ДУХОВНЫЙ ОТЕЦ\n",
      "denominator = 0 for all candidates for key:124758-N synset is СЕРДЦЕБИЕНИЕ\n",
      "lemmaSynset for key 113551-N is none, synset is НАЛЕДЬ, КОРКА ЛЬДА\n",
      "denominator = 0 for all candidates for key:111138-N synset is ТЕЛЬНЯШКА\n",
      "lemma for key 113256-N is not noun, lemmaSynset is ПОТРЕБЛЯТЬ for sysnet ПОТРЕБЛЯТЬ\n",
      "denominator = 0 for all candidates for key:5776-N synset is ДОРОЖНО-ТРАНСПОРТНОЕ ПРОИСШЕСТВИЕ\n",
      "denominator = 0 for all candidates for key:9539-N synset is АБИТУРИЕНТ\n",
      "lemma for key 119941-N is not noun, lemmaSynset is ЧЕЛОВЕЧНЫЙ for sysnet ЧЕЛОВЕЧНЫЙ\n",
      "denominator = 0 for all candidates for key:9325-N synset is РЕКЛАМНЫЙ ЩИТ\n",
      "denominator = 0 for all candidates for key:126246-N synset is ЗАПОЙ (ПЬЯНСТВО)\n",
      "lemma for key 117709-N is not noun, lemmaSynset is ПИТЬ СПИРТНОЕ for sysnet ПИТЬ СПИРТНОЕ\n",
      "denominator = 0 for all candidates for key:5424-N synset is СВЕТОЧУВСТВИТЕЛЬНЫЕ МАТЕРИАЛЫ\n",
      "denominator = 0 for all candidates for key:108422-N synset is КЛУБЕНЬ\n",
      "lemmaSynset for key 4821-N is none, synset is ПОВОЗКА, ГУЖЕВОЙ ТРАНСПОРТ\n",
      "denominator = 0 for all candidates for key:142630-N synset is ФЕДЕРАЛЬНАЯ СЛУЖБА ПО ФИНАНСОВОМУ МОНИТОРИНГУ\n",
      "denominator = 0 for all candidates for key:163692-N synset is ЭЛЬ-ФАЛЛУДЖА\n",
      "denominator = 0 for all candidates for key:152205-N synset is КУПЮРОПРИЕМНИК\n",
      "denominator = 0 for all candidates for key:4343-N synset is АЭРОСЪЕМКА\n",
      "lemmaSynset for key 133799-N is none, synset is ОХРАНЯЕМЫЕ ВИДЫ\n",
      "denominator = 0 for all candidates for key:125979-N synset is ГЯУР (ИНОВЕРЕЦ)\n",
      "lemmaSynset for key 9700-N is none, synset is СЧЕТА БУХГАЛТЕРСКОГО УЧЕТА\n",
      "denominator = 0 for all candidates for key:116216-N synset is АККОМПАНИАТОР\n",
      "denominator = 0 for all candidates for key:151819-N synset is АРТИСТИЧЕСКИЙ КОЛЛЕКТИВ\n",
      "lemmaSynset for key 118203-N is none, synset is РАВНОДУШНЫЙ, БЕЗРАЗЛИЧНЫЙ\n",
      "denominator = 0 for all candidates for key:139775-N synset is СУПЕРШПИОН\n",
      "lemmaSynset for key 128438-N is none, synset is ПРОХВОСТ, ПРОХОДИМЕЦ\n",
      "denominator = 0 for all candidates for key:120936-N synset is ЧУБ (ПРЯДЬ ВОЛОС)\n",
      "denominator = 0 for all candidates for key:10103-N synset is НАКОПИТЕЛЬНЫЙ СЧЕТ\n",
      "denominator = 0 for all candidates for key:129413-N synset is ФАЛЬЦ\n",
      "denominator = 0 for all candidates for key:146992-N synset is ЕВРОИНТЕГРАЦИЯ\n",
      "lemmaSynset for key 129495-N is none, synset is ХОХОТУН, ХОХОТУШКА\n",
      "denominator = 0 for all candidates for key:151376-N synset is ЧЛЕН ПРЕСТУПНОЙ ГРУППИРОВКИ\n",
      "denominator = 0 for all candidates for key:111507-N synset is ТЕРРАРИУМ\n",
      "denominator = 0 for all candidates for key:132448-N synset is ЭЛЕКТРОСВАРКА\n",
      "denominator = 0 for all candidates for key:143299-N synset is ИНВЕСТИЦИИ В ОСНОВНОЙ КАПИТАЛ\n",
      "lemmaSynset for key 105343-N is none, synset is АЛТАЙ, РЕСПУБЛИКА\n",
      "denominator = 0 for all candidates for key:125654-N synset is ПОВЕЛИТЕЛЬ\n",
      "denominator = 0 for all candidates for key:6973-N synset is ЖИЛИЩНО-ЭКСПЛУАТАЦИОННАЯ ОРГАНИЗАЦИЯ\n",
      "lemmaSynset for key 105007-N is none, synset is РУПИЯ, ШРИ-ЛАНКИ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denominator = 0 for all candidates for key:128720-N synset is ПОЛКОВОДЕЦ\n",
      "lemmaSynset for key 128311-N is none, synset is ПРИМИТИВИЗМ, НАИВНОЕ ИСКУССТВО\n",
      "denominator = 0 for all candidates for key:8117-N synset is МОШЕННИК\n",
      "lemmaSynset for key 123618-N is none, synset is НАРУШИТЕЛЬ ПРАВИЛ, ОБЫЧАЕВ\n",
      "denominator = 0 for all candidates for key:126582-N synset is КУЛЬТЯ\n",
      "denominator = 0 for all candidates for key:100203-N synset is УЧУР\n",
      "denominator = 0 for all candidates for key:124906-N synset is АВАНГАРДИСТ (СТОРОННИК АВАНГАРДИЗМА)\n",
      "denominator = 0 for all candidates for key:146220-N synset is АЦИДОФИЛЬНАЯ ПАЛОЧКА\n",
      "lemmaSynset for key 134969-N is none, synset is ЛЕКАРЬ, ЦЕЛИТЕЛЬ\n",
      "denominator = 0 for all candidates for key:6527-N synset is СОТРУДНИК ПРАВООХРАНИТЕЛЬНЫХ ОРГАНОВ\n",
      "denominator = 0 for all candidates for key:116873-N synset is МАЛЕК\n",
      "denominator = 0 for all candidates for key:135630-N synset is ХОРАЛ (ЦЕРКОВНОЕ ПЕСНОПЕНИЕ)\n",
      "denominator = 0 for all candidates for key:7066-N synset is ЮРИДИЧЕСКАЯ ПРАКТИКА\n",
      "denominator = 0 for all candidates for key:1316-N synset is ВАЛЮТНЫЙ КУРС\n",
      "denominator = 0 for all candidates for key:4284-N synset is СПЕЦПОСЕЛЕНИЕ\n",
      "denominator = 0 for all candidates for key:131683-N synset is ЗНАМЕНИТОСТЬ (ИЗВЕСТНЫЙ ЧЕЛОВЕК)\n",
      "denominator = 0 for all candidates for key:126895-N synset is ГИМНАСТКА\n",
      "lemma for key 116579-N is not noun, lemmaSynset is СОЧЕТАТЬСЯ for sysnet СОЧЕТАТЬСЯ (СООТВЕТСТВОВАТЬ)\n",
      "denominator = 0 for all candidates for key:154194-N synset is УКАЗАТЕЛЬ ПОВОРОТА\n",
      "denominator = 0 for all candidates for key:124379-N synset is ХРАБРЕЦ\n",
      "denominator = 0 for all candidates for key:143298-N synset is СПАСАТЕЛЬНАЯ СЛУЖБА\n",
      "lemmaSynset for key 106784-N is none, synset is ПЕРЕЛОМ, ПЕРЕЛОМНЫЙ ЭТАП\n",
      "denominator = 0 for all candidates for key:5067-N synset is ЛОЦМАНСКИЕ РАБОТЫ\n",
      "denominator = 0 for all candidates for key:125278-N synset is МАРМОЗЕТКА\n",
      "denominator = 0 for all candidates for key:148317-N synset is ДОВОДЧИК ДВЕРИ\n",
      "denominator = 0 for all candidates for key:2668-N synset is ПОДДЕЛКА ДОКУМЕНТОВ\n",
      "denominator = 0 for all candidates for key:147506-N synset is ЛЕЧЕБНАЯ ДИЕТА\n",
      "denominator = 0 for all candidates for key:6113-N synset is ПРЕСС-СЛУЖБА\n",
      "denominator = 0 for all candidates for key:121048-N synset is ОЧКО (ЕДИНИЦА СЧЕТА)\n",
      "lemmaSynset for key 141187-N is none, synset is УПРАВЛЕНИЕ КАДРАМИ\n",
      "lemmaSynset for key 144760-N is none, synset is КОКЕР-СПАНИЕЛЬ\n",
      "denominator = 0 for all candidates for key:139458-N synset is СОЗДАНИЕ ПАРТИИ\n",
      "denominator = 0 for all candidates for key:116949-N synset is СТАРЫЙ МУЖЧИНА\n",
      "denominator = 0 for all candidates for key:117804-N synset is ПОЧЕСТИ\n",
      "lemmaSynset for key 140994-N is none, synset is МЕКЛЕНБУРГ - ПЕРЕДНЯЯ ПОМЕРАНИЯ\n",
      "lemmaSynset for key 960-N is none, synset is БУРОВЫЕ РАБОТЫ\n",
      "denominator = 0 for all candidates for key:6345-N synset is СТАНКОСТРОИТЕЛЬНЫЙ ЗАВОД\n",
      "denominator = 0 for all candidates for key:120716-N synset is ЦИМБАЛИСТ\n",
      "denominator = 0 for all candidates for key:165984-N synset is РИФФ\n",
      "lemmaSynset for key 152542-N is none, synset is РУХЛЯДЬ, СТАРЬЕ\n",
      "lemmaSynset for key 148306-N is none, synset is ПРИМОРЦЫ\n",
      "denominator = 0 for all candidates for key:109896-N synset is ЛАФИТ\n",
      "denominator = 0 for all candidates for key:111211-N synset is ЛЫЖНИК\n",
      "denominator = 0 for all candidates for key:3793-N synset is УЧЕБНАЯ ПРОГРАММА\n",
      "denominator = 0 for all candidates for key:127052-N synset is ПОВЕШЕННЫЙ\n",
      "lemma for key 153541-N is not noun, lemmaSynset is встретить хлеб-соль for sysnet ВСТРЕТИТЬ ХЛЕБОМ-СОЛЬЮ\n",
      "denominator = 0 for all candidates for key:145536-N synset is СИЛОВОЙ ЖИМ\n",
      "denominator = 0 for all candidates for key:4493-N synset is АВИАЦИОННЫЙ ПОЛЕТ\n",
      "denominator = 0 for all candidates for key:100436-N synset is ЗИГАН\n",
      "denominator = 0 for all candidates for key:102201-N synset is ТУТОНЧАНА\n",
      "denominator = 0 for all candidates for key:124453-N synset is РОСТОВЩИК\n",
      "denominator = 0 for all candidates for key:141192-N synset is СОЦИАЛЬНАЯ ИДЕНТИЧНОСТЬ\n",
      "lemma for key 109949-N is not noun, lemmaSynset is ЛЕПЕТАТЬ for sysnet ЛЕПЕТАТЬ\n",
      "denominator = 0 for all candidates for key:7681-N synset is СОЛОД\n",
      "denominator = 0 for all candidates for key:115358-N synset is ИМПРОВИЗАТОР\n",
      "denominator = 0 for all candidates for key:124373-N synset is БОГАЧ\n",
      "lemma for key 119310-N is not noun, lemmaSynset is ВНЕЗАПНЫЙ for sysnet ВНЕЗАПНЫЙ\n",
      "denominator = 0 for all candidates for key:154241-N synset is КОФЕ АРАБИКА\n",
      "denominator = 0 for all candidates for key:129211-N synset is СТАРОЖИЛ\n",
      "denominator = 0 for all candidates for key:144600-N synset is СОННИК\n",
      "denominator = 0 for all candidates for key:9721-N synset is САЖЕНЕЦ\n",
      "denominator = 0 for all candidates for key:4340-N synset is ДРЕНАЖНЫЕ РАБОТЫ\n",
      "denominator = 0 for all candidates for key:107246-N synset is ОБРЕЗОК\n",
      "denominator = 0 for all candidates for key:108787-N synset is КАНДИЛЬ\n",
      "denominator = 0 for all candidates for key:4070-N synset is ПАЙЩИК\n",
      "denominator = 0 for all candidates for key:154198-N synset is МОБИЛЬНОЕ УСТРОЙСТВО\n",
      "denominator = 0 for all candidates for key:102280-N synset is АРЦИЗ\n",
      "lemmaSynset for key 104589-N is none, synset is ОДРА\n",
      "lemmaSynset for key 130452-N is none, synset is ВЫМЕРЗНУТЬ, ПОГИБНУТЬ ОТ МОРОЗОВ\n",
      "lemma for key 112821-N is not noun, lemmaSynset is МЕСИТЬ for sysnet МЕСИТЬ\n",
      "lemmaSynset for key 118869-N is none, synset is ПРОСТОЙ, НЕСЛОЖНЫЙ\n",
      "denominator = 0 for all candidates for key:109080-N synset is ПЕРЕДНЯЯ ЧАСТЬ\n"
     ]
    }
   ],
   "source": [
    "#проведем похожий третьему этапу обор кандидатов\n",
    "dictIdTitle = {}\n",
    "i = 0\n",
    "for synset in wn.synsets:\n",
    "    dictIdTitle[synset.id] = synset.title\n",
    "for key, value in dict_wordId_in_display_and_key_new.items():\n",
    "    lemmaSynset = getLemmaForSynsets(wn[str(key)].title.lower())\n",
    "    if lemmaSynset:\n",
    "        idWn = wn.get_senses(lemmaSynset)[0].id \n",
    "        if \"N\" in idWn:\n",
    "            ctxS = dictWn[idWn].ctx\n",
    "            maxP = -1\n",
    "            arg_max = 0\n",
    "            for item in value:\n",
    "                numerator = score(ctxS, item[1].ctxW)\n",
    "                deniminator = 0.0\n",
    "                for num in value:\n",
    "                    deniminator += score(ctxS, num[1].ctxW)\n",
    "                if deniminator != 0.0:\n",
    "                    p = numerator/deniminator\n",
    "                    if p > maxP:\n",
    "                        maxP = p\n",
    "                        arg_max = item\n",
    "            if maxP != -1:\n",
    "                dictDisplay[arg_max[0]] = arg_max[1]\n",
    "            else:\n",
    "                i+=1\n",
    "                print(f\"denominator = 0 for all candidates for key:{key} synset is {wn[str(key)].title}\")\n",
    "        else:\n",
    "            i+=1\n",
    "            print(f\"lemma for key {key} is not noun, lemmaSynset is {lemmaSynset} for sysnet {wn[str(key)].title}\")        \n",
    "    else:\n",
    "        i+=1\n",
    "        print(f\"lemmaSynset for key {key} is none, synset is {wn[str(key)].title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f991a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17777, 316)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictDisplay), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c5a88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay\n",
    "# file = open(\"D:\\\\lbase_data\\\\dictDisplay4.txt\", \"wb\")\n",
    "# pickle.dump(dictDisplay, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay4.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fdb1672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17777"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65735ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 17777/17777 [00:00<00:00, 358746.07it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\fourthpart.txt\",\"w\", encoding=\"utf-8\")\n",
    "for key in tqdm(dictDisplay):\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" wordId: \" + str(dictDisplay[key].wordId) + \" title: \"+str(dictDisplay[key].title) +\" lemma: \"+str(dictDisplay[key].lemma) + '\\n'\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728cb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
