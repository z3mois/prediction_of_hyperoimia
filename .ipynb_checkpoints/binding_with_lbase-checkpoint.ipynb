{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e2fb2d-0092-4f93-80fa-7bd9aa67ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025435425796423165\n"
     ]
    }
   ],
   "source": [
    "from lib2to3.pytree import WildcardPattern\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "import argparse\n",
    "import bz2\n",
    "import logging\n",
    "import os.path\n",
    "import re  # TODO use regex when it will be standard\n",
    "import sys\n",
    "from io import StringIO\n",
    "from multiprocessing import Queue, get_context, cpu_count\n",
    "from timeit import default_timer\n",
    "import re\n",
    "import pymorphy2\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from ruwordnet import RuWordNet\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from  extractor import Extractor\n",
    "import numpy as np\n",
    "import bert\n",
    "from cmT8aq2P import SentenceBertTransformer\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81554125-25c3-4800-b936-7210814d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        if i!= \"(\" and i != \")\":\n",
    "            s +=i\n",
    "        elif i  == \")\":\n",
    "            s += \"\"\n",
    "        else:\n",
    "            s += \",\"\n",
    "    return s\n",
    "@lru_cache(maxsize=200000)\n",
    "def get_normal_form(word):\n",
    "    return morph_analizer.parse(word)[0].normal_form\n",
    "acceptedNamespaces = ['w', 'wiktionary', 'wikt']\n",
    "templateNamespace = ''\n",
    "morph_analizer = pymorphy2.MorphAnalyzer()\n",
    "tagRE = re.compile(r'(.*?)<(/?\\w+)[^>]*>(?:([^<]*)(<.*?>)?)?')\n",
    "def collect_pages(text):\n",
    "    \"\"\"\n",
    "    :param text: the text of a wikipedia file dump.\n",
    "    \"\"\"\n",
    "    # we collect individual lines, since str.join() is significantly faster\n",
    "    # than concatenation\n",
    "    page = []\n",
    "    id = ''\n",
    "    revid = ''\n",
    "    last_id = ''\n",
    "    inText = False\n",
    "    redirect = False\n",
    "    redirect_page = ''\n",
    "    for line in text:\n",
    "        if '<' not in line:     # faster than doing re.search()\n",
    "            if inText:\n",
    "                page.append(line)\n",
    "            continue\n",
    "        m = tagRE.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        tag = m.group(2)\n",
    "        if tag == 'page':\n",
    "            page = []\n",
    "            redirect = False\n",
    "        elif tag == 'id' and not id:\n",
    "            id = m.group(3)\n",
    "        elif tag == 'id' and id: # <revision> <id></id> </revision>\n",
    "            revid = m.group(3)\n",
    "        elif tag == 'title':\n",
    "            title = m.group(3)\n",
    "        elif tag == 'redirect':\n",
    "            redirect = True\n",
    "            redirectRE = re.compile(r'title=\\\"(.*?)\\\" />')\n",
    "            redirect_page = re.findall(redirectRE, line)[0]\n",
    "        elif tag == 'text':\n",
    "            inText = True\n",
    "            line = line[m.start(3):m.end(3)]\n",
    "            page.append(line)\n",
    "            if m.lastindex == 4:  # open-close\n",
    "                inText = False\n",
    "        elif tag == '/text':\n",
    "            if m.group(1):\n",
    "                page.append(m.group(1))\n",
    "            inText = False\n",
    "        elif inText:\n",
    "            page.append(line)\n",
    "        elif tag == '/page':\n",
    "            colon = title.find(':')\n",
    "            if (colon < 0 or (title[:colon] in acceptedNamespaces) and id != last_id and\n",
    "                    not redirect and not title.startswith(templateNamespace)):\n",
    "                yield (id, revid, title, page, redirect_page,redirect)\n",
    "                last_id = id\n",
    "            id = ''\n",
    "            revid = ''\n",
    "            page = []\n",
    "            inText = False\n",
    "            redirect = False\n",
    "            redirect_page=''\n",
    "\n",
    "def decode_open(filename, mode='rt', encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Open a file, decode and decompress, depending on extension `gz`, or 'bz2`.\n",
    "    :param filename: the file to open.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(filename)[1]\n",
    "    if ext == '.gz':\n",
    "        import gzip\n",
    "        return gzip.open(filename, mode, encoding=encoding)\n",
    "    elif ext == '.bz2':\n",
    "        return bz2.open(filename, mode=mode, encoding=encoding)\n",
    "    else:\n",
    "        return open(filename, mode, encoding=encoding)\n",
    "def extract_cat(text):\n",
    "    matcher=re.compile(r\"Категория:\\s?([А-Яа-я\\s?]+)\")\n",
    "    return matcher.findall(text)\n",
    "def extract_links(text):\n",
    "    matcher=re.compile(r\"[\\[\\[]([А-Яа-я\\s?]+)[\\|,\\]\\]]\")\n",
    "    return matcher.findall(text)\n",
    "def extract_first_links(text):\n",
    "    matcher=re.compile(r\"[\\[\\[]([А-Яа-я\\s?]+)[\\|,\\]\\]]\")\n",
    "    answer = []\n",
    "    for elem in text.split(\"\\n\"):\n",
    "        item = matcher.findall(elem)\n",
    "        if len(item) > 0:\n",
    "            answer.append(item[0])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dda5bd4-fd2d-4fbb-8387-b0515260a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Page:\n",
    "    id: int\n",
    "    revid: int\n",
    "    title:str\n",
    "    meaningPage: bool\n",
    "    multiPage: bool\n",
    "    categories: list\n",
    "    links: list\n",
    "    redirect:bool\n",
    "    first_sentence:str\n",
    "    def __eq__(self, other):\n",
    "        return (self.id, self.revid, self.title, self.meaningPage,\n",
    "                self.multiPage, self.categories, self.links, \n",
    "                self.redirect) == (other.id, other.revid, \n",
    "                other.title, other.meaningPage,\n",
    "                other.multiPage, other.categories, other.links, \n",
    "                other.redirect)\n",
    "class PageEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Page):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af38d96d-e401-4bb1-a6e1-14fa2f86a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4432391it [45:31, 1622.56it/s] \n"
     ]
    }
   ],
   "source": [
    "input = decode_open(\"D:\\\\asd\\\\ruwiki-20220701-pages-meta-current.xml.bz2\")\n",
    "i = 0\n",
    "dictRedirect = {}\n",
    "pages = []\n",
    "redirectcount = 0\n",
    "dictPageRedirect = {}\n",
    "# log = open(\"log.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for id, revid, title, page, redirect_page, redirect in tqdm(collect_pages(input)):\n",
    "    # print(title, \"aaaa\",''.join(page), file = printt)\n",
    "    i += 1\n",
    "    text = ''.join(page)\n",
    "    text_lower = text.lower()\n",
    "    multiPage = False\n",
    "    if text_lower.find('{{другие значения') != -1:\n",
    "        multiPage = True\n",
    "    elif title.find(\"(\") != -1 and (not \"значения\" in title.lower())and (not \"значение\" in title.lower()):\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{перенаправление\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{другое значение\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{значения\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{redirect-multi\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{redirect-multi\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{see also\") != -1:\n",
    "        multiPage = True\n",
    "    elif text_lower.find(\"{{о|\") != -1:\n",
    "        multiPage= True\n",
    "    elif text_lower.find(\"{{список однофамильцев}}\") != -1:\n",
    "        multiPage= True\n",
    "    categories = extract_cat(text)\n",
    "    \n",
    "    meaningPage = False\n",
    "    if (\"значения\" in title.lower()) or (\"значение\" in title.lower()):\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{неоднозначность') != -1:\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{многозначность') != -1:\n",
    "        meaningPage = True\n",
    "    elif text_lower.find('{{disambig') != -1:\n",
    "        meaningPage = True\n",
    "    # redirects = extract_redirects(text)\n",
    "    links =[]\n",
    "    if not meaningPage:\n",
    "        links = extract_links(text)\n",
    "    else:\n",
    "        links = extract_first_links(text)\n",
    "    first_sentense = \"\"\n",
    "    if not redirect_page:\n",
    "        ext = Extractor(id,revid,\"\",title,page)\n",
    "        first_sentense = \"\\n\".join(ext.clean_text(text)).split(\".\")[0]\n",
    "        \n",
    "    if len(redirect_page) > 0:\n",
    "        if redirect_page not in dictRedirect:\n",
    "            dictRedirect[redirect_page] = []\n",
    "            dictPageRedirect[redirect_page] = []\n",
    "        dictPageRedirect[redirect_page].append(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect,first_sentense))\n",
    "        dictRedirect[redirect_page].append(title)\n",
    "        redirectcount +=1 \n",
    "    pages.append(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect,first_sentense))\n",
    "    # json.dump(Page(id,revid,title,meaningPage,multiPage,categories,links,redirect),file,cls=PageEncoder,ensure_ascii=False)\n",
    "    # file.write(\"\\n\")\n",
    "#     if title == \"Abbath\" or title == \"Аббате\":\n",
    "#         print(text, file = log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c921fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение  информации о страницах\n",
    "file = open(\"D:\\\\lbase_data\\\\ctxw.txt\", \"wb\")\n",
    "pickle.dump(pages, file=file)\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\drp.txt\", \"wb\")\n",
    "pickle.dump(dictPageRedirect, file=file)\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\dr.txt\", \"wb\")\n",
    "pickle.dump(dictRedirect, file=file)\n",
    "file.close()\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7f51aa-7781-4cc8-a447-3040f44e857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\ctxw.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "pages = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\dr.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictRedirect = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\drp.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictPageRedirect = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06aa9261-4fcb-4ee2-8a1e-66a0dca64bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(pages):\n",
    "    if elem.title == \"Компьютерный язык\":\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381277bd-303c-4488-bed5-dd5fe8ae11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['язык',\n",
       " 'компьютерная техника',\n",
       " 'язык программирования',\n",
       " 'программист',\n",
       " 'компьютер',\n",
       " 'Компьютерный сленг',\n",
       " 'Язык программирования',\n",
       " 'Сценарный язык',\n",
       " 'Псевдоестественный язык',\n",
       " 'Информационный язык',\n",
       " 'Язык разметки',\n",
       " 'Язык спецификаций',\n",
       " 'Каскадные таблицы стилей',\n",
       " 'Язык описания аппаратуры',\n",
       " 'Протокол обмена',\n",
       " 'сетевой протокол',\n",
       " 'Компьютерный сленг']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1340].links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67727d62-1a2b-45bb-94b5-62e4b15d465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WikiSynset():\n",
    "    def __init__(self, page:Page):\n",
    "        self.page = page\n",
    "        self.synset = [page]\n",
    "\n",
    "    def append(self, redirect_title:Page):\n",
    "        self.synset.append(redirect_title)\n",
    "\n",
    "\n",
    "def includeTitleInWn(all_senses, title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_senses:\n",
    "        return True\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_senses:\n",
    "            return True\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word)\n",
    "                for word in text[0].split()])\n",
    "    if lemmatized.upper() in all_senses:\n",
    "        return True\n",
    "    if \"Ё\" in title:\n",
    "        return includeTitleInWn(all_senses, title.replace(\"Ё\",\"Е\"))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119f3970-e227-4156-8bf4-2c222525b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988017\n",
      "988017\n"
     ]
    }
   ],
   "source": [
    "print(len(dictPageRedirect))\n",
    "print(len(dictRedirect))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a387562-36f6-450c-bc9e-4327cd9a233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4432390/4432390 [00:03<00:00, 1396152.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 4432390/4432390 [00:01<00:00, 2974754.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 4432391/4432391 [04:46<00:00, 15477.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki = []\n",
    "meaningPageCounter = 0\n",
    "multiPageCounter = 0\n",
    "includeWS = 0\n",
    "includeTitle = 0\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "all_senses = set([' '.join([get_normal_form(w).upper() for w in s.lemma.split()]) for s in wn.senses])\n",
    "hashDict = {}\n",
    "for index in tqdm(range(len(pages)-1)):\n",
    "    hashDict[pages[index].title.lower()] = index\n",
    "#пройтись по всем значения со страницы-значения и всем значения поставить мульти\n",
    "i = 0\n",
    "for index in tqdm(range(len(pages)-1)):\n",
    "    if pages[index].meaningPage:\n",
    "        for link in pages[index].links:\n",
    "            if link.lower() in hashDict:\n",
    "                pages[hashDict[link.lower()]].multiPage = True\n",
    "                i += 1\n",
    "print(i)\n",
    "for page in tqdm(pages):\n",
    "    if page.redirect:\n",
    "        if includeTitleInWn(all_senses, page.title):\n",
    "            includeTitle += 1\n",
    "        continue\n",
    "    wikiSyn = WikiSynset(page)\n",
    "    if page.title in dictPageRedirect:\n",
    "        for redirect in dictPageRedirect[page.title]:\n",
    "            wikiSyn.append(redirect)\n",
    "    if page.meaningPage:\n",
    "        meaningPageCounter += 1\n",
    "    if page.multiPage:\n",
    "        multiPageCounter += 1\n",
    "    wiki.append(wikiSyn)\n",
    "    if includeTitleInWn(all_senses, page.title):\n",
    "        includeTitle += 1\n",
    "    # else:\n",
    "    #     print(page.title)\n",
    "page = 0\n",
    "print(multiPageCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ab8c90-e5f7-4d47-bf82-1633118ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# includeWS = 0\n",
    "# for wikisyn in tqdm(wiki):\n",
    "#     if includeTitleInWn(all_senses, str(wikisyn.page.title)) and (not wikisyn.page.multiPage) and (not wikisyn.page.meaningPage):\n",
    "#             includeWS += 1\n",
    "#     else:\n",
    "#         for synset in wikisyn.synset:\n",
    "#             if (includeTitleInWn(all_senses, str(synset.title)))and(not synset.multiPage) and (not synset.meaningPage):\n",
    "#                 includeWS += 1\n",
    "#                 break\n",
    "# print(\"count title:\", i)\n",
    "# print(\"count redirect\", redirectcount)\n",
    "# print(\"meaningPage count\", meaningPageCounter)\n",
    "# print(\"multiPage count\", multiPageCounter)\n",
    "# print(\"WikiSynset count\", len(wiki))\n",
    "# print(\"include title in wn count\", includeTitle)\n",
    "# print(\"include solo wikisyn in wn count\", includeWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdfd79e-dc47-435d-b684-03c758b7732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display:\n",
    "    id:int\n",
    "    revid:int\n",
    "    title:str\n",
    "    lemma:str\n",
    "    wordId:int\n",
    "    ctxW:set\n",
    "    first_sentense:str\n",
    "class displayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, display):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9700189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxW(links, categories):\n",
    "    ctx = set()\n",
    "    for link in links:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in link.split()]))\n",
    "    for elem in categories:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in elem.split()]))\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff895bd4-c4bf-474d-8859-fb57b76a6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unambiguousDisplay(title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if len(wn.get_synsets(title)) == 1:\n",
    "        return [True, title]\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if len(wn.get_synsets(text[0])) == 1:\n",
    "            return [True, text[0]]\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word)\n",
    "                for word in text[0].split()])\n",
    "    if len(wn.get_synsets(lemmatized.upper())) == 1:\n",
    "        return [True, lemmatized.upper()]\n",
    "    if \"Ё\" in title:\n",
    "        return unambiguousDisplay(title.replace(\"Ё\",\"Е\"))\n",
    "    return [False, \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c6058a-bbaf-42d5-a9aa-82cf32c72f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #составляем контекст для слова из wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str\n",
    "class WnCtxEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, WnCtx):\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "def getKeyForId(key):\n",
    "    if len(wn.get_senses(key)) > 0:\n",
    "        # print(1)\n",
    "        return wn.get_senses(key)[0].id\n",
    "    if \"(\" in key:\n",
    "        text = my_split(key).split(\",\")\n",
    "        if  len(wn.get_senses(text[0])) > 0:\n",
    "            # print(2)\n",
    "            return wn.get_senses(text[0])[0].id\n",
    "    text = my_split(key).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if len(wn.get_senses(lemmatized)) > 0:\n",
    "        # print(3)\n",
    "        return wn.get_senses(lemmatized)[0].id\n",
    "    if \"ё\" in key:\n",
    "        return getKeyForId(key.replace(\"ё\",\"у\"))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3855035c-6a0b-4ccc-93db-8f3b2e37032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ЯЗЫК ПРОГРАММИРОВАНИЕ', 'ИСКУССТВЕННЫЙ ЯЗЫК', 'ЭСПЕРАНТО', 'КОМПЬЮТЕРНЫЙ ЯЗЫК', 'ЯЗЫК ПРОГРАММИРОВАНИЯ'}\n"
     ]
    }
   ],
   "source": [
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "print(extractCtxS(wn, 'КОМПЬЮТЕРНЫЙ ЯЗЫК'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604a34b2-59cc-413d-94e6-edc15d94e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 4/94710 [00:00<4:23:47,  5.98it/s]Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 1696, in _fetchall_impl\n",
      "    return list(self.iterator)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\", line 147, in chunks\n",
      "    fetch = cursor._raw_all_rows()\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\", line 392, in _raw_all_rows\n",
      "    rows = self._fetchall_impl()\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 1804, in _fetchall_impl\n",
      "    return self.cursor_strategy.fetchall(self, self.cursor)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 981, in fetchall\n",
      "    self.handle_exception(result, dbapi_cursor, e)\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 941, in handle_exception\n",
      "    result.connection._handle_dbapi_exception(\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2047, in _handle_dbapi_exception\n",
      "    util.raise_(exc_info[1], with_traceback=exc_info[2])\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\", line 208, in raise_\n",
      "    raise exception\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\", line 977, in fetchall\n",
      "    rows = dbapi_cursor.fetchall()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Professional\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1914, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "sqlite3.ProgrammingError: Cannot operate on a closed database.\n",
      "  0%|                                                                              | 4/94710 [00:00<5:21:45,  4.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5672\\3349740056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtext_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lemma\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mctx_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractCtxS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mctx_s\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5672\\3181374621.py\u001b[0m in \u001b[0;36mextractCtxS\u001b[1;34m(wn, lemma)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#synonymy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msense\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_synsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msynonymy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msenses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mctx_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynonymy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#Hypernymy/Hyponymy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    479\u001b[0m                     \u001b[0mreplace_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                 )\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, state, dict_, passive)\u001b[0m\n\u001b[0;32m    939\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mPASSIVE_NO_RESULT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_loader_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mPASSIVE_NO_RESULT\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNO_VALUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\attributes.py\u001b[0m in \u001b[0;36m_fire_loader_callables\u001b[1;34m(self, state, key, passive)\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mATTR_EMPTY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\strategies.py\u001b[0m in \u001b[0;36m_load_for_state\u001b[1;34m(self, state, passive, loadopt, extra_criteria)\u001b[0m\n\u001b[0;32m    909\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPASSIVE_NO_RESULT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         return self._emit_lazyload(\n\u001b[0m\u001b[0;32m    912\u001b[0m             \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\strategies.py\u001b[0m in \u001b[0;36m_emit_lazyload\u001b[1;34m(self, session, state, primary_key_identity, passive, loadopt, extra_criteria)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         )\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muselist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \"\"\"\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_allrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_allrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mmake_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmake_row\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mmade_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_real_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchmany_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1696\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1697\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\orm\\loading.py\u001b[0m in \u001b[0;36mchunks\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mfetch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_all_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msingle_entity\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_raw_all_rows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_all_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mmake_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchmany_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mhandle_exception\u001b[1;34m(self, result, dbapi_cursor, err)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhandle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         result.connection._handle_dbapi_exception(\n\u001b[0m\u001b[0;32m    942\u001b[0m             \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                 )\n\u001b[0;32m   2046\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py\u001b[0m in \u001b[0;36mfetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbapi_cursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#read word in wn\n",
    "mydoc = minidom.parse('D:\\\\lbase_data\\\\senses.N.xml')\n",
    "items = mydoc.getElementsByTagName('sense')\n",
    "countWn = 0\n",
    "dictWn = {}\n",
    "for elem in tqdm(items):\n",
    "    countWn +=1\n",
    "    text = elem.attributes['name'].value\n",
    "    text_id = elem.attributes[\"id\"].value\n",
    "    lemma = elem.attributes[\"lemma\"].value\n",
    "    ctx_s = extractCtxS(wn, lemma)\n",
    "    ctx = set()\n",
    "    for elem in ctx_s:\n",
    "        ctx.add(\" \".join([get_normal_form(word) for word in elem.split()]))\n",
    "    dictWn[text_id] = WnCtx(text_id, ctx, lemma, text)\n",
    "print(len(dictWn), countWn)\n",
    "print(wn.get_synsets(\"замок\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e848d55-82d4-44f1-a546-ce50c419be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"D:\\\\lbase_data\\\\ctxS.txt\", \"wb\")\n",
    "# pickle.dump(dictWn, file=file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "753dc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb09a45b-9b8a-4714-a488-11b72deb9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WnCtx(id='130542-N-196389', ctx={'корректурный правка', 'тщательно проверить', 'выверить', 'вычитка текст', 'редактирование текст', 'модерация контент', 'вычитывание', 'вычитка'}, lemmaInWn='ВЫЧИТКА', name='ВЫЧИТКА')\n"
     ]
    }
   ],
   "source": [
    "# print(dictRedirect[\"Аббат\"])\n",
    "print(dictWn[\"130542-N-196389\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd212af-0a94-4204-915f-ff1cc3519368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLemmaForSynsets(title):\n",
    "    title = title.upper()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    ch = \" \" \n",
    "    if len(wn.get_synsets(title))>0:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if len(wn.get_synsets(text[0]))>0:\n",
    "            if text[0][len(text[0]) - 1] == \" \":\n",
    "                text[0] = text[0].rstrip(ch)\n",
    "            return text[0]\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if len(wn.get_synsets(lemmatized))>0:\n",
    "        if lemmatized[len(lemmatized) - 1] == \" \":\n",
    "            lemmatized = lemmatized.rstrip(ch)\n",
    "        return lemmatized\n",
    "    if \"Ё\" in title:\n",
    "        return getLemmaForSynsets(title.replace(\"Ё\",\"Е\"))\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddcaf586-21ee-460b-a402-6d8334a882af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1826284/1826284 [12:34<00:00, 2420.66it/s]\n"
     ]
    }
   ],
   "source": [
    "dictDisplay = {} # словарь отображений\n",
    "new_wiki = [] #будут все викисинсеты, кроме однозначных\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "for wikisyn in tqdm(wiki):\n",
    "    if len(wikisyn.synset) != 1:\n",
    "        new_wiki.append(wikisyn)\n",
    "        continue\n",
    "    one = unambiguousDisplay(wikisyn.page.title)\n",
    "    if  (not wikisyn.page.meaningPage) and (not wikisyn.page.multiPage) and (getKeyForId(wikisyn.page.title) in dictWn) and one[0] :\n",
    "        #id\n",
    "        temp = wn.get_synsets(one[1])[0].id\n",
    "        dictDisplay[wikisyn.page.title] = (display(wikisyn.page.id,wikisyn.page.revid,wikisyn.page.title,one[1], temp,\n",
    "                                                  extractCtxW(wikisyn.page.links, wikisyn.page.categories), wikisyn.page.first_sentence))\n",
    "    else:\n",
    "        new_wiki.append(wikisyn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7088c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay  и new_wiki\n",
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay1.txt\", \"wb\")\n",
    "pickle.dump(dictDisplay, file=file)\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\new_wiki.txt\", \"wb\")\n",
    "pickle.dump(new_wiki, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f6d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay1.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\new_wiki.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "new_wiki = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b3f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     запись резулаов первого этапа в удобночиаемый вид\n",
    "file = open(\"D:\\\\lbase_data\\\\unambiguousDisplay.txt\", \"w\")\n",
    "for key in dictDisplay:\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" revid: \" + str(dictDisplay[key].revid) + \" title: \"+str(dictDisplay[key].title)  + \" lemma: \"  + str(dictDisplay[key].lemma) + \" wordId:\" + str(dictDisplay[key].wordId)+\"\\n\"\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06b7501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "031327db-d7fb-4d12-a4c6-59919b1a02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1823875/1823875 [48:59<00:00, 620.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#чуть раньше собираем кандидатов, для более точной\n",
    "#проверки многозначности страниц\n",
    "dictLemmaInIndex = {}\n",
    "for index in tqdm(range(len(new_wiki))):\n",
    "    lemma = getLemmaForSynsets(new_wiki[index].page.title)\n",
    "    if lemma != \"\":\n",
    "        if not lemma in dictLemmaInIndex:\n",
    "            dictLemmaInIndex[lemma] = []\n",
    "        dictLemmaInIndex[lemma].append(index)\n",
    "    else:\n",
    "        for elem in new_wiki[index].synset:\n",
    "            lemma1 = getLemmaForSynsets(elem.title)\n",
    "            if lemma1 != \"\":\n",
    "                if not lemma1 in dictLemmaInIndex:\n",
    "                    dictLemmaInIndex[lemma1] = []\n",
    "                dictLemmaInIndex[lemma1].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a4fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictLemmaInIndex\n",
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex.txt\", \"wb\")\n",
    "pickle.dump(dictLemmaInIndex, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da947d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictLemmaInIndex = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fe3a03-0ebe-4603-bd81-dde335be4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1823875/1823875 [00:01<00:00, 1535248.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16957\n"
     ]
    }
   ],
   "source": [
    "#дополнительная проверка на многозначность\n",
    "dictTitleInIndex = {}\n",
    "for index in tqdm(range(len(new_wiki))):\n",
    "    dictTitleInIndex[new_wiki[index].page.title] = index\n",
    "tempCount = 0\n",
    "for key in dictLemmaInIndex:\n",
    "    for elem in dictLemmaInIndex[key]:\n",
    "        if not new_wiki[elem].page.multiPage:\n",
    "            if len(dictLemmaInIndex[key])>1:\n",
    "                new_wiki[dictTitleInIndex[new_wiki[elem].page.title]].page.multiPage = True\n",
    "                tempCount +=1\n",
    "print(tempCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678ba80f-5329-407f-9a1c-03966e28ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823875\n",
      "2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1823875/1823875 [24:56<00:00, 1218.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7427 5018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def helpForCheck(d, title):\n",
    "    d = d.upper()\n",
    "    d = d.replace(\"—\", \"-\")\n",
    "    d = d.replace(\",\", \"\")\n",
    "    if d == title:\n",
    "        return True\n",
    "    if \"(\" in d:\n",
    "        text = my_split(d).split(\",\")\n",
    "        if title == text[0]:\n",
    "            return True\n",
    "    text = my_split(d).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    if lemmatized == title:\n",
    "        return True\n",
    "    if \"Ё\" in d:\n",
    "        return helpForCheck(d.replace(\"Ё\",\"Е\"), d)\n",
    "    return False\n",
    "def check(wn, lemma, d):\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        if helpForCheck(d, sense.title):\n",
    "            return [True, sense]\n",
    "    return [False, \"ф\"]\n",
    "print(len(new_wiki))\n",
    "print(len(dictDisplay))\n",
    "wiki3 = [] # будут только многозначные\n",
    "# for w in tqdm(new_wiki):\n",
    "#     if w.page.title == \"Плебеи\":\n",
    "#         print(w.page.title, w.page.multiPage)\n",
    "#         print(unambiguousDisplay(w.page.title))\n",
    "#         print(getKeyForId(w.page.title))\n",
    "#         break\n",
    "countN = 0\n",
    "for w in tqdm(new_wiki):\n",
    "    flag = False\n",
    "    if not w.page.meaningPage and not w.page.multiPage:\n",
    "        for d in w.synset:\n",
    "            one = unambiguousDisplay(d.title)\n",
    "            #d.title in dictDisplay and\n",
    "            if  one[0] and \"N\" in wn.get_synsets(one[1])[0].id:\n",
    "                if w.page.title == \"Abbath\":\n",
    "                    print(d.title)\n",
    "                flag = True\n",
    "                countN += 1\n",
    "                idd = wn.get_synsets(one[1])[0].id\n",
    "                p = display(w.page.id,w.page.revid,w.page.title,one[1], idd,\n",
    "                            extractCtxW(w.page.links, w.page.categories), w.page.first_sentence)\n",
    "                dictDisplay[w.page.title]=p\n",
    "                break\n",
    "    if not flag:\n",
    "        wiki3.append(w)\n",
    "print(len(dictDisplay), countN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a851d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\secondpart.txt\", \"w\", encoding=\"utf-8\")\n",
    "for key in dictDisplay:\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" revid: \" + str(dictDisplay[key].revid) + \" title: \"+str(dictDisplay[key].title)  + \" lemma: \"  + str(dictDisplay[key].lemma) + \" wordId:\" + str(dictDisplay[key].wordId)+\"\\n\"\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay  и wiki3\n",
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay2.txt\", \"wb\")\n",
    "pickle.dump(dictDisplay, file=file)\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\wiki3.txt\", \"wb\")\n",
    "pickle.dump(wiki3, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27de5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay2.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()\n",
    "file = open(\"D:\\\\lbase_data\\\\wiki3.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "wiki3 = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeecf420-1e2b-491f-a527-8e9dc4a082f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1818857/1818857 [48:35<00:00, 623.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#теперь собираем кандидатов только из оставшихся статей\n",
    "dictLemmaInIndex = {}\n",
    "for index in tqdm(range(len(wiki3))):\n",
    "    lemma = getLemmaForSynsets(wiki3[index].page.title)\n",
    "    if lemma != \"\":\n",
    "        if not lemma in dictLemmaInIndex:\n",
    "            dictLemmaInIndex[lemma] = []\n",
    "        dictLemmaInIndex[lemma].append(index)\n",
    "    else:\n",
    "        for elem in wiki3[index].synset:\n",
    "            lemma1 = getLemmaForSynsets(elem.title)\n",
    "            if lemma1 != \"\":\n",
    "                if not lemma1 in dictLemmaInIndex:\n",
    "                    dictLemmaInIndex[lemma1] = []\n",
    "                dictLemmaInIndex[lemma1].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9012962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictLemmaInIndex\n",
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex1.txt\", \"wb\")\n",
    "pickle.dump(dictLemmaInIndex, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ed58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictLemmaInIndex1.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictLemmaInIndex = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "932677fb-e093-43a5-8ca9-9e91d69df074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 145/1818857 [00:00<42:02, 720.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ВОЛГА 1\n",
      "АЙОВА 2\n",
      "АЙОВА 3\n",
      "АЙОВА 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1818857/1818857 [22:16<00:00, 1360.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#собираем кандидатов\n",
    "from collections import defaultdict\n",
    "dictSynsetId = defaultdict(list)\n",
    "countLinksAdd = 0\n",
    "wn = RuWordNet(filename_or_session='D:\\\\lbase_data\\\\ruwordnet.db')\n",
    "for index in tqdm(range(len(wiki3))):\n",
    "    w = wiki3[index]\n",
    "    if not w.page.meaningPage:\n",
    "        lemma = getLemmaForSynsets(w.page.title)\n",
    "        #если нашли лемму из ворднета и она существительное\n",
    "        if lemma != \"\" and getKeyForId(lemma) in dictWn:\n",
    "            for synset in wn.get_synsets(lemma):\n",
    "                dictSynsetId[synset.id].append(w)\n",
    "    else:\n",
    "        lemmaTitle = getLemmaForSynsets(w.page.title)\n",
    "        # if lemmaTitle.lower() == \"тихий океан\":\n",
    "        #     print(1111, index, w.page.links, w.page.title, w.page.meaningPage)\n",
    "        for link in w.page.links:\n",
    "            lemma = getLemmaForSynsets(link)\n",
    "            if lemma in dictLemmaInIndex:\n",
    "                #если нашли лемму многозначной статьи из ворднета и она существительное\n",
    "                if lemmaTitle != \"\" and getKeyForId(lemmaTitle) in dictWn:\n",
    "                    for synset in wn.get_synsets(lemmaTitle):\n",
    "                        countLinksAdd += 1\n",
    "                        if countLinksAdd < 5:\n",
    "                            print(lemma,countLinksAdd)\n",
    "                        for indexElem in dictLemmaInIndex[lemma]:\n",
    "                            dictSynsetId[synset.id].append(wiki3[indexElem])\n",
    "print(countLinksAdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e380c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictSynsetId\n",
    "file = open(\"D:\\\\lbase_data\\\\dictSynsetId.txt\", \"wb\")\n",
    "pickle.dump(dictSynsetId, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd1bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictSynsetId.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictSynsetId = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be0a4857-233a-4b36-bf18-e5ce6254d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20502/20502 [00:01<00:00, 14365.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20502/20502 [00:00<00:00, 67663.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#возможны повторения среди кандидатов, уадлим их\n",
    "def count1(value, array):\n",
    "    count = 0\n",
    "    for elem in array:\n",
    "        if elem.page.id == value.page.id:\n",
    "            count += 1\n",
    "    return count\n",
    "for key in tqdm(dictSynsetId):\n",
    "    tempList = []\n",
    "    for elem in dictSynsetId[key]:\n",
    "        if count1(elem, tempList) < 1 and not elem.page.meaningPage:\n",
    "            tempList.append(elem)\n",
    "    dictSynsetId[key] = tempList\n",
    "#sort candidates\n",
    "sorted_tuple = sorted(dictSynsetId.items(), key=lambda x: x[0])\n",
    "sortCandidates = dict(sorted_tuple)\n",
    "dictIdTitle = {}\n",
    "for synset in wn.synsets:\n",
    "    dictIdTitle[synset.id] = synset.title\n",
    "#запись в файл претендентов\n",
    "candidates = open(\"D:\\\\lbase_data\\\\candidates.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for key in tqdm(sortCandidates):\n",
    "    s = \"id synset = \"+str(key) + \" synset title = \" + dictIdTitle[key]\n",
    "    for elem in sortCandidates[key]:\n",
    "        s += \" title = \" + elem.page.title +\" id = \" + elem.page.id + \" \"\n",
    "    print(s, file = candidates)\n",
    "candidates.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5d0debe-7d09-466b-b2ba-a3e0ddd0cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(ctxS, ctxW):\n",
    "    return len(ctxS.intersection(ctxW))\n",
    "def get_key(key, dictWn):\n",
    "    key = key.lower()\n",
    "    key = key.replace(\"—\", \"-\")\n",
    "    key = key.replace(\",\", \"\")\n",
    "    id = getKeyForId(key)\n",
    "    if id in dictWn:\n",
    "        # print(1)\n",
    "        return id\n",
    "    if \"(\" in key:\n",
    "        id = wn.get_senses(text[0])[0].id\n",
    "        text = my_split(key).split(\",\")\n",
    "        if  id in dictWn:\n",
    "            # print(2)\n",
    "            return id\n",
    "    text = my_split(key).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word) for word in text[0].split()])\n",
    "    id = wn.get_senses(lemmatized)[0].id\n",
    "    if id in dictWn:\n",
    "        # print(3)\n",
    "        return id\n",
    "    if \"ё\" in key:\n",
    "        return get_key(key.replace(\"ё\",\"у\"), dictWn)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87b2175a-4b6c-4419-bd9b-736feb980eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labse = SentenceBertTransformer(device=\"cuda\")\n",
    "labse.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79190696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosin_distance(word, sentense):\n",
    "    word_embeding = labse.transform(word)\n",
    "    sentense_embeding = labse.transform(sentense)\n",
    "    return np.dot(word_embeding, sentense_embeding) / (sum(sentense_embeding ** 2) * sum(word_embeding ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a1e4b25-500a-4678-a3e8-518530d36d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20502/20502 [31:04<00:00, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dictDisplay) 22313\n",
      "len(badlemma) 14130\n",
      "len(baddenominator) 18086\n",
      "len(badmaxP) 2415\n",
      "len(badsynsetlemma) 1638\n",
      "len(badidWn) 738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#3 этап алгоритма\n",
    "badlemma = []\n",
    "baddenominator = []\n",
    "badmaxP = []\n",
    "badsynsetlemma = []\n",
    "badidWn = []\n",
    "print(len(dictDisplay))\n",
    "dictSortCandidates = {}\n",
    "for key in tqdm(sortCandidates):\n",
    "    if len(sortCandidates[key]) == 1:\n",
    "            w = sortCandidates[key][0]\n",
    "            p = display(w.page.id,w.page.revid,w.page.title,dictIdTitle[key], key,extractCtxW(w.page.links, w.page.categories), w.page.first_sentence)\n",
    "            dictDisplay[w.page.title]=p\n",
    "            dictSortCandidates[key] = [(sortCandidates[key][0], 1)]\n",
    "    else:\n",
    "        maxP = -1\n",
    "        maxagrument = 0\n",
    "        lemmaSynset = getLemmaForSynsets(dictIdTitle[key])\n",
    "        dictSortCandidates[key] = []\n",
    "        if lemmaSynset != \"\":\n",
    "            idWn = wn.get_senses(lemmaSynset)[0].id\n",
    "            if \"N\" in idWn: #почему-то иногда для синсета существительного сенс не существительное\n",
    "                for elem in sortCandidates[key]:\n",
    "                    ctxw = extractCtxW(elem.page.links, elem.page.categories)\n",
    "                    lemma = getLemmaForSynsets(elem.page.title)\n",
    "                    if lemma != \"\":\n",
    "                        numerator = score(dictWn[idWn].ctx, ctxw)\n",
    "                        denominator = 0\n",
    "                        for item in sortCandidates[key]:\n",
    "                            addctxw = extractCtxW(item.page.links, item.page.categories)\n",
    "                            denominator +=score(dictWn[idWn].ctx, addctxw)\n",
    "                    else:\n",
    "                        badlemma.append(elem.page.title)\n",
    "                    temp_distance = cosin_distance(lemmaSynset, elem.page.first_sentence)\n",
    "                    if denominator != 0:\n",
    "                        temp_p = numerator / denominator +  temp_distance \n",
    "                        dictSortCandidates[key].append((elem, temp_p))                           \n",
    "                        if  temp_p > maxP:\n",
    "                            maxP = temp_p\n",
    "                            maxagrument = elem\n",
    "                    else:\n",
    "                        if  temp_distance > maxP:\n",
    "                            maxP = temp_distance\n",
    "                            maxagrument = elem\n",
    "                        baddenominator.append(elem.page.title)\n",
    "                        dictSortCandidates[key].append((elem, temp_distance))\n",
    "            else:\n",
    "                badidWn.append(wn.get_senses(lemmaSynset)[0].id)\n",
    "        else:\n",
    "            badsynsetlemma.append(dictIdTitle[key])\n",
    "        if maxP != - 1:\n",
    "            w = maxagrument\n",
    "            p = display(w.page.id,w.page.revid,w.page.title,dictIdTitle[key], key,\n",
    "                        extractCtxW(w.page.links, w.page.categories), w.page.first_sentence)\n",
    "            dictDisplay[w.page.title]=p\n",
    "        else:\n",
    "            badmaxP.append(key)\n",
    "print(\"len(dictDisplay)\",len(dictDisplay)) \n",
    "print(\"len(badlemma)\",len(badlemma))\n",
    "print(\"len(baddenominator)\",len(baddenominator))\n",
    "print(\"len(badmaxP)\",len(badmaxP))\n",
    "print(\"len(badsynsetlemma)\",len(badsynsetlemma))\n",
    "print(\"len(badidWn)\",len(badidWn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fa6104a-607a-4f1b-97b5-26b73f333b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20502/20502 [01:12<00:00, 281.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in dictSortCandidates:\n",
    "    temp = sorted(dictSortCandidates[key],key=lambda x: x[1], reverse = True)\n",
    "    dictSortCandidates[key] = temp\n",
    "candidates = open(\"D:\\\\lbase_data\\\\sortcandidates.txt\", \"w\", encoding=\"utf-8\")\n",
    "candidates1 = open(\"D:\\\\lbase_data\\\\sortcandidates1000.txt\", \"w\", encoding=\"utf-8\")\n",
    "i = 0\n",
    "for key in tqdm(dictSortCandidates):\n",
    "    s = \"id synset: \"+str(key) + \"\\tsynset title: \" + dictIdTitle[key] + \"\\n\" \n",
    "    lemmaSynset = getLemmaForSynsets(dictIdTitle[key])\n",
    "    if lemmaSynset != \"\" :\n",
    "        idWn = wn.get_senses(lemmaSynset)[0].id\n",
    "        if \"N\" in idWn:\n",
    "            s += \"Ctx synset: \" + \",\".join(list(dictWn[idWn].ctx)) + \"\\n\"\n",
    "        else:\n",
    "            s += \"id for N is A. lemma: \" + lemmaSynset + \"\\n\"\n",
    "    else:\n",
    "        s += \"lemmasynset not found\\n\" \n",
    "    for elem in dictSortCandidates[key]:\n",
    "        s += \"title: \" + elem[0].page.title +\" id = \" + elem[0].page.id + \" \" + str(elem[1]) + \"\\n\"\n",
    "        s += \"ctx article:\" +\",\".join(list(extractCtxW(elem[0].page.links,elem[0].page.categories))) +\"\\n\"\n",
    "    s += \"\\n\"\n",
    "    print(s, file = candidates)\n",
    "    if i < 1000:\n",
    "        print(s, file = candidates1) \n",
    "    i += 1\n",
    "candidates.close()\n",
    "candidates1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb572f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay\n",
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay3.txt\", \"wb\")\n",
    "pickle.dump(dictDisplay, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc02f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay3.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81fbb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 22313/22313 [00:00<00:00, 429099.71it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\thirdpart.txt\",\"w\", encoding=\"utf-8\")\n",
    "for key in tqdm(dictDisplay):\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" wordId: \" + str(dictDisplay[key].wordId) + \" title: \"+str(dictDisplay[key].title) +\" lemma: \"+str(dictDisplay[key].lemma) + '\\n'\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eea0f596-8063-492a-ba25-f32490084ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4603, 19878, 22313)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#собираем отображения  связанные содним wordId\n",
    "dict_wordId_in_display_and_key = defaultdict(list)\n",
    "for key, value in dictDisplay.items():\n",
    "    dict_wordId_in_display_and_key[value.wordId].append((key,value))\n",
    "i  = 0\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        i+=len(value)\n",
    "i, len(dict_wordId_in_display_and_key), len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75628e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 17710)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удалим из dictDisplay повторяющиеся wordId\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        for item in value:\n",
    "            del dictDisplay[item[0]]\n",
    "#удалим из dict_wordId_in_display_and_key однозначные отображенния\n",
    "dict_wordId_in_display_and_key_new = defaultdict(list)\n",
    "for key, value in dict_wordId_in_display_and_key.items():\n",
    "    if len(value) > 1:\n",
    "        dict_wordId_in_display_and_key_new[key] = value\n",
    "len(dict_wordId_in_display_and_key_new), len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cade63db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Компьютерный язык',\n",
       "  display(id='2661', revid='2184205', title='Компьютерный язык', lemma='КОМПЬЮТЕРНЫЙ ЯЗЫК', wordId='7651-N', ctxW={'каскадный таблица стиль', 'язык разметка', 'компьютер', 'язык описание аппаратура', 'компьютерный техника', 'язык спецификация', 'протокол обмен', 'информационный язык', 'компьютерный сленг', 'компьютерный язык', 'язык', 'псевдоестественный язык', 'сетевой протокол', 'сценарный язык', 'язык программирование', 'программист'}, first_sentense='Понятие компью́терный язы́к (калька с ), как правило, относится к языкам, ассоциируемым с компьютерной техникой')),\n",
       " ('Язык программирования',\n",
       "  display(id='2728', revid='121215610', title='Язык программирования', lemma='ЯЗЫК ПРОГРАММИРОВАНИЯ', wordId='7651-N', ctxW={'симула', 'логика первый порядок', 'системный программирование', 'зависимый тип', 'компьютер', 'приведение тип', 'планкалкюль', 'тест производительность', 'утечка память', 'стандарт оформление код', 'мемоизация', 'хоар', 'структурный программирование', 'исполнимый модуль', 'алгол', 'сериализация', 'интерпретатор', 'механический пианино', 'исходный код', 'музыкальный шкатулка', 'регистр процессор', 'кобол', 'статический типизация', 'серебряный пуля нет', 'теория язык программирование', 'качество по', 'теория категория', 'визуальный программирование', 'человеческий фактор', 'структура данные', 'цуза', 'статический анализ код', 'гомоморфизм', 'прототипный программирование', 'компьютерный программа', 'компилировать язык программирование', 'япония', 'каламбур типизация', 'субд', 'метакласс', 'айверсон', 'бэббидж', 'реляционный исчисление', 'гомоиконичность', 'объект первый класс', 'турчин', 'архитектура процессор', 'система модуль', 'язык программирование', 'лавров', 'программист', 'программирование', 'исчисление кортеж', 'кэй', 'деление на ноль', 'поисковый система', 'транслятор', 'брайан керниган', 'тьюринг', 'типобезопасность', 'процедурный программирование', 'миллиметр', 'вычислительный процесс', 'сопоставление с образец', 'чистота язык программирование', 'символьный вычисление', 'аксиоматический семантика', 'композиция функция', 'число бернулли', 'оптимизация хвостовой вызов', 'структура и интерпретация компьютерный программа', 'конкатенативный язык программирование', 'дюйм', 'информатика', 'гласс', 'аппаратный платформа компьютер', 'формальный язык', 'лисп', 'межпроцедурный оптимизация', 'переписывание', 'высокий доступность', 'обработка исключение', 'шаблон проектирование', 'лавлейс', 'модель актор', 'ссср', 'математический логика', 'стековый язык', 'исчисление процесс', 'функция первый класс', 'ссылочный прозрачность', 'компьютерный язык', 'оптимизировать компилятор', 'полиморфизм роды', 'формальный верификация', 'управление память на основа регион', 'машинный язык', 'машина тьюринг', 'фортран', 'добросвета', 'метапрограммирование', 'контрактный программирование', 'джон мокнуть', 'тип данные', 'фредерик брукс', 'эзотерический язык программирование', 'параллельный вычисление', 'выведение тип', 'система тип', 'подпрограмма', 'кнорусый', 'мейнстрим', 'поколение язык программирование', 'безопасность доступ к память', 'сборка мусор', 'интуиционистский теория тип', 'параметрический полиморфизм', 'документация на программный обеспечение', 'целый тип', 'теория автомат', 'область видимость', 'лексика', 'марков', 'модульный программирование', 'интернет', 'функциональный программирование', 'динамический типизация', 'низкоуровневый язык программирование', 'ассемблер', 'анализ поток управление', 'алгоритм', 'язык программирование высокий уровень', 'интерфейс', 'программный средство', 'полнота по тьюринг', 'брукс', 'виртуализация', 'милнер', 'метаязык', 'международный стандартизация', 'спецификация язык программирование', 'качество программный обеспечение', 'естественный язык', 'архитектура фон нейман', 'стратегия вычисление', 'функция высокий порядок', 'крах программный обеспечение', 'конструктор тип', 'проектирование программный обеспечение', 'теория множество', 'комбинаторный логика', 'машинный код', 'раскраска граф', 'программирование ограничение', 'язык ассемблер', 'макрокоманда', 'алгоритмический неразрешимость', 'регулярный выражение', 'операционный семантика', 'императивный программирование', 'виртуальный машина', 'сигнатура функция', 'интерфейс командный строка', 'учебный язык программирование', 'логический программирование', 'жаккардовый ткацкий станок', 'класс тип', 'бестиповыя язык', 'суперкомпиляция', 'повторный использование код', 'регулярный язык', 'оптимизация хвостовой рекурсия', 'дейкстра', 'шестибитный кодировка', 'игрушечный язык', 'сша', 'комбинатор', 'строковый тип', 'безопасность язык', 'теория порядок', 'декларативный программирование', 'рефал', 'база данные', 'порождать программирование', 'сильный и слабый типизация', 'грамматика с фразовый структура', 'логика высокий порядок', 'автоматический доказательство', 'нормальный алгоритм', 'автокод', 'интерпретировать язык программирование', 'денотационный семантика', 'вирт', 'операционный система', 'юникод', 'полиморфизм в высокий роды', 'исчисление предикат', 'процессорный время', 'компилятор'}, first_sentense='Язы́к программи́рования\\xa0— формальный язык, предназначенный для записи компьютерных программ'))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_wordId_in_display_and_key_new[\"7651-N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b6231ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2168/2168 [00:51<00:00, 42.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#проведем похожий третьему этапу обор кандидатов\n",
    "dictIdTitle = {}\n",
    "i = 0\n",
    "for synset in wn.synsets:\n",
    "    dictIdTitle[synset.id] = synset.title\n",
    "for key, value in tqdm(dict_wordId_in_display_and_key_new.items()):\n",
    "    lemmaSynset = getLemmaForSynsets(wn[str(key)].title.lower())\n",
    "    if lemmaSynset:\n",
    "        idWn = wn.get_senses(lemmaSynset)[0].id \n",
    "        if \"N\" in idWn:\n",
    "            ctxS = dictWn[idWn].ctx\n",
    "            maxP = -1\n",
    "            arg_max = 0\n",
    "            for item in value:\n",
    "                numerator = score(ctxS, item[1].ctxW)\n",
    "                deniminator = 0.0\n",
    "                for num in value:\n",
    "                    deniminator += score(ctxS, num[1].ctxW)\n",
    "                p = cosin_distance(lemmaSynset, item[1].first_sentense)\n",
    "                if deniminator != 0.0:\n",
    "                    p += numerator/deniminator\n",
    "                if p > maxP:\n",
    "                    maxP = p\n",
    "                    arg_max = item\n",
    "            if maxP != -1:\n",
    "                dictDisplay[arg_max[0]] = arg_max[1]\n",
    "            else:\n",
    "                i+=1\n",
    "#                 print(f\"denominator = 0 for all candidates for key:{key} synset is {wn[str(key)].title}\")\n",
    "        else:\n",
    "            i+=1\n",
    "#             print(f\"lemma for key {key} is not noun, lemmaSynset is {lemmaSynset} for sysnet {wn[str(key)].title}\")        \n",
    "    else:\n",
    "        i+=1\n",
    "#         print(f\"lemmaSynset for key {key} is none, synset is {wn[str(key)].title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f991a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19775, 103)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictDisplay), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c5a88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение dictDisplay\n",
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay4.txt\", \"wb\")\n",
    "pickle.dump(dictDisplay, file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\dictDisplay4.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictDisplay = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3fdb1672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19775"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65735ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 19775/19775 [00:00<00:00, 520406.33it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"D:\\\\lbase_data\\\\fourthpart.txt\",\"w\", encoding=\"utf-8\")\n",
    "for key in tqdm(dictDisplay):\n",
    "    temp = \"id:  \"+str(dictDisplay[key].id) + \" wordId: \" + str(dictDisplay[key].wordId) + \" title: \"+str(dictDisplay[key].title) +\" lemma: \"+str(dictDisplay[key].lemma) + '\\n'\n",
    "    file.write(temp)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d728cb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>annotation</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Да</td>\n",
       "      <td>Облава</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шаньдун</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Да</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Да</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шахматная доска</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1993</td>\n",
       "      <td>113701-N</td>\n",
       "      <td>Царю небесный</td>\n",
       "      <td>Нет</td>\n",
       "      <td>Бог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1994</td>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Да</td>\n",
       "      <td>Наставничество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>1995</td>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Да</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>1996</td>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Да</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1998</td>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Да</td>\n",
       "      <td>Землянка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 synset_id              wiki_title annotation  \\\n",
       "0              0  115195-N                  Облава         Да   \n",
       "1              1  128158-N                 Шаньдун         Да   \n",
       "2              2  143699-N               Оксюморон         Да   \n",
       "3              3    4061-N            Доверенность         Да   \n",
       "4              5  136544-N         Шахматная доска         Да   \n",
       "...          ...       ...                     ...        ...   \n",
       "1725        1993  113701-N           Царю небесный        Нет   \n",
       "1726        1994  124807-N          Наставничество         Да   \n",
       "1727        1995    9199-N  Трансурановые элементы         Да   \n",
       "1728        1996  103410-N                Эсватини         Да   \n",
       "1729        1998  141998-N                Землянка         Да   \n",
       "\n",
       "             wiki_title_gold  \n",
       "0                     Облава  \n",
       "1                    Шаньдун  \n",
       "2                  Оксюморон  \n",
       "3               Доверенность  \n",
       "4            Шахматная доска  \n",
       "...                      ...  \n",
       "1725                     Бог  \n",
       "1726          Наставничество  \n",
       "1727  Трансурановые элементы  \n",
       "1728                Эсватини  \n",
       "1729                Землянка  \n",
       "\n",
       "[1730 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:\\\\lbase_data\\\\wiki_merge_predict_test_29_11_22.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9799b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_id = set(df[\"synset_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28891b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19775/19775 [00:00<00:00, 1977454.74it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_for_check = {}\n",
    "for key, value in tqdm(dictDisplay.items()):\n",
    "    if value.wordId in synset_id:\n",
    "        dict_for_check[value.wordId] = value.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "323495d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_2312\\3563258175.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Облава</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Шаньдун</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>не связан</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>Бог</td>\n",
       "      <td>Бог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Наставничество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Землянка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold              wiki_title\n",
       "0     115195-N                  Облава                  Облава\n",
       "1     128158-N                 Шаньдун                 Шаньдун\n",
       "2     143699-N               Оксюморон               Оксюморон\n",
       "3       4061-N            Доверенность            Доверенность\n",
       "4     136544-N         Шахматная доска               не связан\n",
       "...        ...                     ...                     ...\n",
       "1725  113701-N                     Бог                     Бог\n",
       "1726  124807-N          Наставничество          Наставничество\n",
       "1727    9199-N  Трансурановые элементы  Трансурановые элементы\n",
       "1728  103410-N                Эсватини                Эсватини\n",
       "1729  141998-N                Землянка                Землянка\n",
       "\n",
       "[1730 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df[[\"synset_id\", \"wiki_title_gold\"]]\n",
    "for_apply = df[\"synset_id\"]\n",
    "dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ee7ae30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7416184971098266"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dataset[\"wiki_title\"].values\n",
    "y_true = dataset[\"wiki_title_gold\"].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bf71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
